---
title: "Comentarios de Usuarios de Reddit sobre Herramientas de Chat LLM Principales"
tags: [IA, ChatGPT, Claude, Google Gemini, LLMs de Código Abierto]
keywords: [herramientas de chat IA, comentarios de usuarios, ChatGPT, Claude, Google Gemini, LLMs de código abierto, análisis de Reddit]
authors: [lark]
description: Este artículo proporciona un análisis profundo de las discusiones en Reddit sobre herramientas de chat IA populares, incluyendo ChatGPT, Claude, Google Gemini y LLMs de código abierto. Destaca los puntos de dolor reportados por los usuarios, las características frecuentemente solicitadas y las necesidades desatendidas, ofreciendo perspectivas sobre las fortalezas y debilidades de cada herramienta.
image: "https://opengraph-image.blockeden.xyz/api/og-cuckoo-network?title=Comentarios%20de%20Usuarios%20de%20Reddit%20sobre%20Herramientas%20de%20Chat%20LLM%20Principales"
---

# Comentarios de Usuarios de Reddit sobre Herramientas de Chat LLM Principales

**Visión General:** Este informe analiza las discusiones en Reddit sobre cuatro herramientas de chat IA populares – **ChatGPT de OpenAI**, **Claude de Anthropic**, **Gemini (Bard) de Google** y **LLMs de código abierto** (por ejemplo, modelos basados en LLaMA). Resume los puntos de dolor comunes que los usuarios reportan para cada uno, las características que solicitan con más frecuencia, las necesidades no satisfechas o segmentos de usuarios que se sienten desatendidos, y las diferencias en la percepción entre desarrolladores, usuarios casuales y usuarios empresariales. Se incluyen ejemplos específicos y citas de hilos de Reddit para ilustrar estos puntos.

![Comentarios de Usuarios de Reddit sobre Herramientas de Chat LLM Principales](https://opengraph-image.blockeden.xyz/api/og-cuckoo-network?title=Comentarios%20de%20Usuarios%20de%20Reddit%20sobre%20Herramientas%20de%20Chat%20LLM%20Principales)

## ChatGPT (OpenAI)

### Puntos de Dolor Comunes y Limitaciones

- **Memoria de contexto limitada:** Una de las principales quejas es la incapacidad de ChatGPT para manejar conversaciones largas o documentos grandes sin olvidar detalles anteriores. Los usuarios frecuentemente alcanzan el límite de longitud de contexto (unos pocos miles de tokens) y deben truncar o resumir información. Un usuario señaló *“aumentar el tamaño de la ventana de contexto sería de lejos la mayor mejora... Ese es el límite con el que me encuentro más”*. Cuando se excede el contexto, ChatGPT olvida las instrucciones o el contenido inicial, lo que lleva a caídas frustrantes en la calidad a mitad de sesión.

- **Límites de mensajes para GPT-4:** Los usuarios de ChatGPT Plus lamentan el límite de 25 mensajes/3 horas en el uso de GPT-4 (un límite presente en 2023). Al alcanzar este límite, se ven obligados a esperar, interrumpiendo su trabajo. Los usuarios intensivos encuentran esta restricción como un gran punto de dolor.

- **Filtros de contenido estrictos (“nerfs”):** Muchos Redditors sienten que ChatGPT se ha vuelto demasiado restrictivo, a menudo rechazando solicitudes que versiones anteriores manejaban. Un post muy votado se quejó de que *“prácticamente cualquier cosa que le pidas hoy en día devuelve un ‘Lo siento, no puedo ayudarte’... ¿Cómo pasó de ser la herramienta más útil al equivalente de Google Assistant?”*. Los usuarios citan ejemplos como ChatGPT negándose a reformatear *su propio* texto (por ejemplo, credenciales de inicio de sesión) debido a un uso indebido hipotético. Los suscriptores de pago argumentan que *“alguna vaga noción de que el usuario puede hacer cosas 'malas'... no debería ser motivo para no mostrar resultados”*, ya que quieren la salida del modelo y la usarán responsablemente.

- **Alucinaciones y errores:** A pesar de su capacidad avanzada, ChatGPT puede producir información incorrecta o fabricada con confianza. Algunos usuarios han observado que esto empeora con el tiempo, sospechando que el modelo fue “simplificado”. Por ejemplo, un usuario en finanzas dijo que ChatGPT solía calcular métricas como NPV o IRR correctamente, pero después de las actualizaciones *“estoy obteniendo tantas respuestas incorrectas... todavía produce respuestas incorrectas [incluso después de la corrección]. Realmente creo que se ha vuelto mucho más tonto desde los cambios.”*. Estas inexactitudes impredecibles erosionan la confianza para tareas que requieren precisión factual.

- **Salidas de código incompletas:** Los desarrolladores a menudo usan ChatGPT para ayuda con la codificación, pero informan que a veces omite partes de la solución o trunca código largo. Un usuario compartió que ChatGPT ahora *“omite código, produce código poco útil, y simplemente apesta en lo que necesito que haga... A menudo omite tanto código que ni siquiera sé cómo integrar su solución.”* Esto obliga a los usuarios a hacer preguntas de seguimiento para obtener el resto, o a unir manualmente las respuestas, un proceso tedioso.

- **Preocupaciones de rendimiento y tiempo de actividad:** Existe la percepción de que el rendimiento de ChatGPT para usuarios individuales disminuyó a medida que aumentó el uso empresarial. *“Creo que están asignando ancho de banda y poder de procesamiento a las empresas y quitándoselo a los usuarios, lo cual es insoportable considerando lo que cuesta una suscripción!”* opinó un suscriptor Plus frustrado. Se han notado anecdóticamente interrupciones o ralentizaciones durante los momentos de mayor actividad, lo que puede interrumpir los flujos de trabajo.

### Características o Mejoras Solicitadas Frecuentemente

- **Ventana de contexto/memoria más larga:** De lejos, la mejora más solicitada es una mayor longitud de contexto. Los usuarios quieren tener conversaciones mucho más largas o alimentar documentos grandes sin reinicios. Muchos sugieren expandir el contexto de ChatGPT para igualar la capacidad de 32K tokens de GPT-4 (actualmente disponible a través de API) o más. Como dijo un usuario, *“GPT es mejor con contexto, y cuando no recuerda ese contexto inicial, me frustro... Si los rumores son ciertos sobre los PDFs de contexto, eso resolvería básicamente todos mis problemas.”* Hay una gran demanda de características para cargar documentos o vincular datos personales para que ChatGPT pueda recordarlos y referenciarlos durante una sesión.

- **Manejo de archivos e integración:** Los usuarios frecuentemente piden formas más fáciles de alimentar archivos o datos en ChatGPT. En las discusiones, las personas mencionan querer *“copiar y pegar mi Google Drive y que funcione”* o tener complementos que permitan a ChatGPT obtener directamente contexto de archivos personales. Algunos han intentado soluciones alternativas (como complementos de lector de PDF o vinculación de Google Docs), pero se quejan de errores y límites. Un usuario describió su complemento ideal como uno que *“funciona como Link Reader pero para archivos personales... eligiendo qué partes de mi unidad usar en una conversación... eso resolvería básicamente todos los problemas que tengo con GPT-4 actualmente.”*. En resumen, el mejor soporte nativo para el conocimiento externo (más allá de los datos de entrenamiento) es una solicitud popular.

- **Reducción de la limitación para usuarios de pago:** Dado que muchos usuarios Plus alcanzan el límite de mensajes de GPT-4, piden límites más altos o una opción para pagar más por acceso ilimitado. El límite de 25 mensajes se ve como arbitrario y obstaculiza el uso intensivo. Las personas preferirían un modelo basado en el uso o un límite más alto para que las sesiones largas de resolución de problemas no se corten.

- **Modos de moderación “sin censura” o personalizados:** Un segmento de usuarios desearía la capacidad de alternar la estrictitud de los filtros de contenido, especialmente al usar ChatGPT para ellos mismos (no contenido público). Sienten que un modo de “investigación” o “sin censura” – con advertencias pero no rechazos duros – les permitiría explorar más libremente. Como señaló un usuario, los clientes que pagan lo ven como una herramienta y creen *“Pago dinero por [ello].”* Quieren la opción de obtener respuestas incluso en consultas límite. Mientras OpenAI debe equilibrar la seguridad, estos usuarios sugieren una bandera o configuración para relajar las políticas en chats privados.

- **Mejora de la precisión factual y actualizaciones:** Los usuarios comúnmente piden un conocimiento más actualizado y menos alucinaciones. El corte de conocimiento de ChatGPT (septiembre de 2021 en versiones anteriores) fue una limitación a menudo planteada en Reddit. OpenAI ha introducido navegación y complementos, que algunos usuarios aprovechan, pero otros simplemente solicitan que el modelo base se actualice más frecuentemente con nuevos datos. Reducir errores obvios – especialmente en dominios como matemáticas y codificación – es un deseo continuo. Algunos desarrolladores proporcionan retroalimentación cuando ChatGPT comete errores con la esperanza de mejorar el modelo.

- **Mejores salidas de código y herramientas:** Los desarrolladores tienen solicitudes de características como un intérprete de código mejorado que no omita contenido, e integración con IDEs o control de versiones. (El complemento Code Interpreter de OpenAI – ahora parte de “Análisis de Datos Avanzado” – fue un paso en esta dirección y recibió elogios.) Aún así, los usuarios a menudo solicitan un control más fino en la generación de código: por ejemplo, una opción para generar código completo y sin filtrar incluso si es largo, o mecanismos para corregir fácilmente el código si la IA cometió un error. Básicamente, quieren que ChatGPT se comporte más como un asistente de codificación confiable sin necesidad de múltiples indicaciones para refinar la respuesta.

- **Perfiles de usuario persistentes o memoria:** Otra mejora que algunos mencionan es permitir que ChatGPT recuerde cosas sobre el usuario entre sesiones (con consentimiento). Por ejemplo, recordar el estilo de escritura de uno, o que son ingenieros de software, sin tener que repetirlo en cada nuevo chat. Esto podría vincularse con el ajuste fino de la API o una función de “perfil”. Los usuarios copian manualmente el contexto importante en nuevos chats ahora, por lo que una memoria incorporada para preferencias personales ahorraría tiempo.

### Necesidades Desatendidas o Segmentos de Usuarios

- **Investigadores y estudiantes con documentos largos:** Las personas que quieren que ChatGPT analice artículos de investigación extensos, libros o grandes conjuntos de datos se sienten desatendidas. Los límites actuales los obligan a dividir el texto o conformarse con resúmenes. Este segmento se beneficiaría enormemente de ventanas de contexto más grandes o características para manejar documentos largos (como lo evidencian numerosos posts sobre intentar superar los límites de tokens).

- **Usuarios que buscan narración creativa o juego de roles más allá de los límites:** Aunque ChatGPT se usa a menudo para escritura creativa, algunos narradores se sienten limitados por el modelo olvidando puntos de la trama tempranos en una historia larga o rechazando contenido adulto/terror. Recurrieron a modelos alternativos o trucos para continuar sus narrativas. Estos usuarios creativos estarían mejor servidos por una versión de ChatGPT con memoria más larga y un poco más de flexibilidad en violencia ficticia o temas maduros (dentro de lo razonable). Como señaló un escritor de ficción, cuando la IA pierde el hilo de la historia, *“Tengo que recordarle el formato o contexto exacto... Me frustra que fuera genial hace dos indicaciones, pero ahora tengo que poner al día a la IA.”*.

- **Usuarios avanzados y expertos en dominios:** Los profesionales en campos especializados (**finanzas**, **ingeniería**, **medicina**) a veces encuentran que las respuestas de ChatGPT carecen de profundidad o precisión en su dominio, especialmente si las preguntas involucran desarrollos recientes. Estos usuarios desean un conocimiento experto más confiable. Algunos han intentado el ajuste fino a través de la API o GPTs personalizados. Aquellos que no pueden ajustar fino apreciarían versiones específicas de dominio de ChatGPT o complementos que integren bases de datos confiables. En su forma predeterminada, ChatGPT puede desatender a los usuarios que necesitan información altamente precisa y específica de campo (a menudo tienen que verificar su trabajo).

- **Usuarios que necesitan contenido sin censura o de casos límite:** Una minoría de usuarios (hackers probando escenarios de seguridad, escritores de ficción extrema, etc.) encuentran que las restricciones de contenido de ChatGPT son demasiado limitantes para sus necesidades. Actualmente están desatendidos por el producto oficial (ya que evita explícitamente cierto contenido). Estos usuarios a menudo experimentan con indicaciones de fuga o usan modelos de código abierto para obtener las respuestas que desean. Esta es una brecha deliberada para OpenAI (para mantener la seguridad), pero significa que tales usuarios buscan en otro lugar.

- **Individuos y empresas preocupados por la privacidad:** Algunos usuarios (especialmente en entornos corporativos) se sienten incómodos enviando datos sensibles a ChatGPT debido a preocupaciones de privacidad. OpenAI tiene políticas para no usar datos de la API para entrenamiento, pero la interfaz web de ChatGPT históricamente no ofrecía tales garantías hasta que se agregó una función de exclusión. Las empresas que manejan datos confidenciales (legales, de salud, etc.) a menudo sienten que no pueden utilizar completamente ChatGPT, dejando sus necesidades desatendidas a menos que construyan soluciones autoalojadas. Por ejemplo, un Redditor mencionó que su empresa se mudó a un LLM local por razones de privacidad. Hasta que estén disponibles instancias locales o privadas de ChatGPT, este segmento sigue siendo cauteloso o utiliza proveedores más pequeños y especializados.

### Diferencias en la Percepción por Tipo de Usuario

- **Desarrolladores/Usuarios Técnicos:** Los desarrolladores tienden a ser tanto algunos de los mayores defensores de ChatGPT como sus críticos más duros. Aman su capacidad para explicar código, generar plantillas y ayudar en la depuración. Sin embargo, sienten agudamente sus limitaciones en contexto más largo y precisión de código. Como se quejó un desarrollador, ChatGPT comenzó a *“producir código poco útil”* y omitir partes importantes, lo que *“me molesta... No quiero tener que decirle ‘no seas perezoso’ – solo quiero el resultado completo”*. Los desarrolladores a menudo notan incluso cambios sutiles en la calidad después de actualizaciones del modelo y han sido muy vocales en Reddit sobre los “nerfs” percibidos o las caídas en la capacidad de codificación. También empujan los límites (construyendo indicaciones complejas, encadenando herramientas), por lo que anhelan características como contexto expandido, menos límites de mensajes y mejor integración con herramientas de codificación. En resumen, los desarrolladores valoran ChatGPT por acelerar tareas rutinarias pero son rápidos en señalar errores en lógica o código – lo ven como un asistente junior que aún necesita supervisión.

- **Usuarios Casuales/Cotidianos:** Los usuarios más casuales – aquellos que piden conocimiento general, consejos o diversión – a menudo se maravillan de las capacidades de ChatGPT, pero tienen sus propias quejas. Una frustración común de los usuarios casuales es cuando ChatGPT rechaza una solicitud que les parece inocua (probablemente activando una regla de política). El autor original en un hilo ejemplificó esto, estando *“tan molesto cuando escribo una indicación que no debería tener problema y ahora se niega”*. Los usuarios casuales también pueden encontrarse con el corte de conocimiento (descubriendo que el bot no puede manejar eventos muy actuales a menos que se actualice explícitamente) y a veces notan cuando ChatGPT da una respuesta obviamente incorrecta. A diferencia de los desarrolladores, es posible que no siempre verifiquen la IA, lo que puede llevar a decepción si actúan sobre un error. En el lado positivo, muchos usuarios casuales encuentran que las respuestas más rápidas de ChatGPT Plus y la salida mejorada de GPT-4 valen $20/mes – a menos que el problema de “rechazo” u otros límites arruinen la experiencia. Generalmente quieren un asistente útil y todo propósito y pueden frustrarse cuando ChatGPT responde con declaraciones de política o necesita una indicación compleja para obtener una respuesta simple.

- **Usuarios Empresariales/Profesionales:** Los usuarios empresariales a menudo abordan ChatGPT desde un punto de vista de productividad y fiabilidad. Aprecian el borrador rápido de correos electrónicos, resúmenes de documentos o generación de ideas. Sin embargo, les preocupa la **seguridad de los datos**, la consistencia y la integración en flujos de trabajo. En Reddit, los profesionales han discutido querer ChatGPT en herramientas como Outlook, Google Docs o como una API en sus sistemas internos. Algunos han notado que a medida que OpenAI pivota para servir a clientes empresariales, el enfoque del producto parece cambiar: hay una sensación de que la experiencia del usuario gratuito o individual se degradó ligeramente (por ejemplo, más lenta o “menos inteligente”) a medida que la empresa escaló para servir a clientes más grandes. Ya sea cierto o no, destaca una percepción: los usuarios empresariales quieren fiabilidad y servicio prioritario, y los usuarios individuales temen ser ahora de segunda clase. Además, los profesionales necesitan salidas correctas – una respuesta llamativa pero incorrecta puede ser peor que ninguna respuesta. Por lo tanto, este segmento es sensible a la precisión. Para ellos, características como contexto más largo (para leer contratos, analizar bases de código) y tiempo de actividad garantizado son cruciales. Es probable que paguen más por niveles de servicio premium, siempre que se cumplan sus requisitos de cumplimiento y privacidad. Algunas empresas incluso exploran implementaciones locales o usan la API de OpenAI con reglas estrictas de manejo de datos para satisfacer sus políticas de TI.

---

## Claude (Anthropic)

### Puntos de Dolor Comunes y Limitaciones

- **Límites de uso y restricciones de acceso:** Claude recibió elogios por ofrecer un modelo poderoso (Claude 2) de forma gratuita, pero los usuarios rápidamente encontraron límites de uso (especialmente en el nivel gratuito). Después de cierto número de indicaciones o una gran cantidad de texto, Claude puede detenerse y decir algo como *“Lo siento, tengo que concluir esta conversación por ahora. Por favor regresa más tarde.”* Esta limitación frustra a los usuarios que tratan a Claude como un socio extendido de codificación o escritura. Incluso los usuarios de Claude Pro (de pago) no están *“garantizados tiempo ilimitado”*, como señaló un usuario; al alcanzar la cuota todavía produce el mensaje de “regresa más tarde”. Además, durante mucho tiempo Claude estuvo oficialmente georrestringido (inicialmente solo disponible en EE. UU./Reino Unido). Los usuarios internacionales en Reddit tuvieron que usar VPNs o plataformas de terceros para acceder a él, lo que fue un inconveniente. Esto hizo que muchos usuarios fuera de EE. UU. se sintieran excluidos hasta que se amplió el acceso.

- **Tendencia a desviarse con entradas muy grandes:** La característica principal de Claude es su *ventana de contexto de 100k tokens*, permitiendo indicaciones extremadamente largas. Sin embargo, algunos usuarios han notado que cuando se introducen decenas de miles de tokens en Claude, sus respuestas pueden volverse menos enfocadas. *“100k es súper útil pero si no sigue las instrucciones correctamente y se desvía, no es tan útil,”* observó un usuario. Esto sugiere que con contextos enormes, Claude podría desviarse o comenzar a divagar, requiriendo indicaciones cuidadosas para mantenerlo en tarea. Es una limitación inherente a llevar el contexto al extremo – el modelo retiene mucho pero a veces “olvida” qué detalles son más relevantes, llevando a alucinaciones menores o tangentes fuera de tema.

- **Formato inconsistente u obediencia a instrucciones:** En comparaciones lado a lado, algunos usuarios encontraron a Claude menos predecible en cómo sigue ciertas directrices. Por ejemplo, Claude se describe como *“más humano en las interacciones. Pero sigue menos estrictamente los mensajes del sistema.”*. Esto significa que si le das un formato fijo a seguir o una persona muy estricta, Claude podría desviarse más que ChatGPT. Los desarrolladores que dependen de salidas deterministas (como formatos JSON o estilos específicos) a veces se frustran si Claude introduce comentarios adicionales o no se adhiere rígidamente a la plantilla.

- **Restricciones de contenido y rechazos:** Aunque no es tan frecuentemente criticado como los de ChatGPT, los filtros de seguridad de Claude sí aparecen. Anthropic diseñó a Claude con un fuerte énfasis en la IA constitucional (haciendo que la IA siga directrices éticas). Los usuarios generalmente encuentran a Claude dispuesto a discutir una amplia gama de temas, pero hay instancias donde Claude rechaza solicitudes que ChatGPT podría permitir. Por ejemplo, un Redditor notó *“ChatGPT tiene menos restricciones morales... explicará qué máscaras de gas son mejores para qué condiciones mientras Claude se negará”*. Esto sugiere que Claude podría ser más estricto sobre ciertos consejos “sensibles” (quizás tratándolo como una guía potencialmente peligrosa). Otro usuario intentó un escenario de juego de roles lúdico (“finge que fuiste abducido por extraterrestres”) que Claude rechazó, mientras que Gemini y ChatGPT participarían. Entonces, Claude tiene filtros que pueden sorprender ocasionalmente a los usuarios que esperan que sea más permisivo.

- **Falta de capacidades multimodales:** A diferencia de ChatGPT (que, a fines de 2023, ganó comprensión de imágenes con GPT-4 Vision), Claude es actualmente solo de texto. Los usuarios de Reddit notan que Claude no puede analizar imágenes o navegar por la web por sí solo. Esto no es exactamente un “punto de dolor” (Anthropic nunca anunció esas características), pero es una limitación en comparación con los competidores. Los usuarios que quieren que una IA interprete un diagrama o captura de pantalla no pueden usar Claude para eso, mientras que ChatGPT o Gemini podrían manejarlo. De manera similar, cualquier recuperación de información actual requiere usar Claude a través de una herramienta de terceros (por ejemplo, integración con Poe o motores de búsqueda), ya que Claude no tiene un modo de navegación oficial en este momento.

- **Problemas menores de estabilidad:** Algunos usuarios han informado que Claude ocasionalmente es repetitivo o se queda atascado en bucles para ciertas indicaciones (aunque esto es menos común que con algunos modelos más pequeños). Además, las versiones anteriores de Claude a veces terminaban respuestas prematuramente o tardaban mucho con salidas grandes, lo que puede verse como pequeñas molestias, aunque Claude 2 mejoró en velocidad.

### Características o Mejoras Solicitadas Frecuentemente

- **Límites de uso más altos o ajustables:** Los entusiastas de Claude en Reddit a menudo piden a Anthropic que aumente los límites de conversación. Les gustaría usar el contexto de 100k en su máxima expresión sin alcanzar un límite artificial. Algunos sugieren que incluso Claude Pro de pago debería permitir *significativamente* más tokens por día. Otros flotaron la idea de un “modo extendido de 100k” opcional – por ejemplo, *“Claude debería tener un modo de contexto de 100k con el doble de los límites de uso”* – donde quizás una suscripción podría ofrecer acceso ampliado para usuarios intensivos. En esencia, hay demanda de un plan que compita con el uso ilimitado (o de alto límite) de ChatGPT para suscriptores.

- **Mejor navegación de contexto largo:** Aunque tener 100k tokens es innovador, los usuarios quieren que Claude *utilice* mejor ese contexto. Una mejora sería refinar cómo Claude prioriza la información para que se mantenga en el camino. Anthropic podría trabajar en la adherencia del modelo a la indicación cuando la indicación es enorme. Las discusiones en Reddit sugieren técnicas como permitir al usuario “fijar” ciertas instrucciones para que no se diluyan en un contexto grande. Cualquier herramienta para ayudar a segmentar o resumir partes de la entrada también podría ayudar a Claude a manejar entradas grandes de manera más coherente. En resumen, a los usuarios les encanta la posibilidad de alimentar un libro completo a Claude – solo quieren que se mantenga agudo durante todo el proceso.

- **Complementos o navegación web:** Muchos usuarios de ChatGPT se han acostumbrado a los complementos (por ejemplo, navegación, ejecución de código, etc.) y expresan interés en que Claude tenga una extensibilidad similar. Una solicitud común es que Claude tenga una función oficial de búsqueda/navegación web, para que pueda obtener información actualizada a demanda. Actualmente, el conocimiento de Claude es mayormente estático (datos de entrenamiento hasta principios de 2023, con algunas actualizaciones). Si Claude pudiera consultar la web, aliviaría esa limitación. Del mismo modo, un sistema de complementos donde Claude pudiera usar herramientas de terceros (como calculadoras o conectores de bases de datos) podría expandir su utilidad para usuarios avanzados. Esto sigue siendo una característica que falta en Claude, y los usuarios de Reddit a menudo mencionan cómo el ecosistema de complementos de ChatGPT le da una ventaja en ciertas tareas.

- **Entrada multimodal (imágenes o audio):** Algunos usuarios también se han preguntado si Claude admitirá entradas de imágenes o generará imágenes. Google’s Gemini y GPT-4 de OpenAI tienen capacidades multimodales, por lo que para mantenerse competitivo, los usuarios esperan que Anthropic explore esto. Una solicitud frecuente es: *“¿Puedo subir un PDF o una imagen para que Claude la analice?”* Actualmente la respuesta es no (aparte de soluciones alternativas como convertir imágenes a texto en otro lugar). Incluso solo permitir imagen a texto (OCR y descripción) satisfaría a muchos que quieren un asistente todo en uno. Esto está en la lista de deseos, aunque Anthropic no ha anunciado nada similar a principios de 2025.

- **Ajuste fino o personalización:** Los usuarios avanzados y las empresas a veces preguntan si pueden ajustar fino a Claude en sus propios datos u obtener versiones personalizadas. OpenAI ofrece ajuste fino para algunos modelos (aún no para GPT-4, pero para GPT-3.5). Anthropic lanzó una interfaz de ajuste fino para Claude 1.3 anteriormente, pero no se anuncia ampliamente para Claude 2. Los usuarios de Reddit han preguntado sobre poder entrenar a Claude en el conocimiento de la empresa o el estilo de escritura personal. Una forma más fácil de hacer esto (además de inyecciones de indicaciones cada vez) sería muy bienvenida, ya que podría convertir a Claude en un asistente personalizado que recuerda una base de conocimiento o persona específica.

- **Disponibilidad más amplia:** Los usuarios fuera de EE. UU. frecuentemente solicitan que Claude se lance oficialmente en sus países. Publicaciones de Canadá, Europa, India, etc., preguntan cuándo podrán usar el sitio web de Claude sin una VPN o cuándo la API de Claude estará abierta más ampliamente. Anthropic ha sido cauteloso, pero la demanda es global – probablemente una mejora a los ojos de muchos sería simplemente “dejar que más de nosotros lo usemos.” La expansión gradual del acceso por parte de la empresa ha abordado parcialmente esto.

### Necesidades Desatendidas o Segmentos de Usuarios

- **Base de usuarios internacional:** Como se mencionó, durante mucho tiempo la base de usuarios principal de Claude estuvo limitada por la geografía. Esto dejó a muchos *posibles* usuarios desatendidos. Por ejemplo, un desarrollador en Alemania interesado en el contexto de 100k de Claude no tenía forma oficial de usarlo. Si bien existen soluciones alternativas (plataformas de terceros, o VPN + verificación telefónica en un país compatible), estas barreras significaban que los usuarios internacionales casuales estaban efectivamente bloqueados. En contraste, ChatGPT está disponible en la mayoría de los países. Entonces, los angloparlantes no estadounidenses y especialmente los no angloparlantes han sido desatendidos por el lanzamiento limitado de Claude. Pueden seguir confiando en ChatGPT o modelos locales simplemente debido a problemas de acceso.

- **Usuarios que necesitan un formato de salida estricto:** Como se mencionó, Claude a veces toma libertades en las respuestas. Los usuarios que necesitan salidas altamente estructuradas (como JSON para una aplicación, o una respuesta siguiendo un formato preciso) podrían encontrar a Claude menos confiable para eso que ChatGPT. Estos usuarios – a menudo desarrolladores que integran la IA en un sistema – son un segmento que podría ser mejor servido si Claude permitiera un “modo estricto” o mejorara su adherencia a las instrucciones. Actualmente podrían evitar a Claude para tales tareas, quedándose con modelos conocidos por seguir formatos más rígidamente.

- **Usuarios casuales de preguntas y respuestas (vs. usuarios creativos):** Claude a menudo es elogiado por tareas creativas – produce prosa fluida, similar a la humana, y ensayos reflexivos. Sin embargo, algunos usuarios en Reddit notaron que para preguntas-respuestas directas o consultas factuales, Claude a veces da respuestas verbosas donde la brevedad sería suficiente. El usuario que comparó ChatGPT y Claude dijo que ChatGPT tiende a ser conciso y con viñetas, mientras que Claude da más narrativa por defecto. Los usuarios que solo quieren una respuesta factual rápida (como “¿Cuál es la capital de X y su población?”) podrían sentir que Claude es un poco indirecto. Estos usuarios están mejor servidos por algo como una búsqueda precisa o un modelo conciso. Claude puede hacerlo si se le pide, pero su estilo puede no coincidir con la expectativa de una preguntas-respuestas concisa, lo que significa que este segmento podría recurrir a otras herramientas (como Bing Chat o Google).

- **Usuarios críticos de seguridad:** Por el contrario, algunos usuarios que *requieren* una adherencia muy cuidadosa a la seguridad (por ejemplo, educadores que usan IA con estudiantes, o clientes empresariales que quieren cero riesgo de salidas descontroladas) podrían considerar la alineación de Claude un plus, pero dado que ChatGPT también está bastante alineado y tiene más características empresariales, esos usuarios podrían no elegir específicamente a Claude. Es un segmento pequeño, pero uno podría argumentar que Claude aún no lo ha capturado distintamente. Pueden estar desatendidos en el sentido de que no tienen una manera fácil de *aumentar* las salvaguardas de Claude o ver su “cadena de pensamiento” (que Anthropic tiene internamente a través del enfoque de IA constitucional, pero los usuarios finales no interactúan directamente con eso aparte de notar el tono generalmente educado de Claude).

- **Hablantes no ingleses (calidad de salida):** Claude fue entrenado principalmente en inglés (como la mayoría de los grandes LLMs). Algunos usuarios lo han probado en otros idiomas; puede responder en muchos, pero la calidad puede variar. Si, por ejemplo, un usuario quiere una respuesta muy matizada en francés o hindi, es posible que las habilidades de Claude no estén tan afinadas allí como las de ChatGPT (GPT-4 ha demostrado un rendimiento multilingüe fuerte, a menudo más alto que otros modelos en ciertos puntos de referencia). Los usuarios que conversan principalmente en idiomas distintos al inglés podrían encontrar la fluidez o precisión de Claude ligeramente más débil. Este segmento está algo desatendido simplemente porque Anthropic no ha destacado el entrenamiento multilingüe como una prioridad públicamente.

### Diferencias en la Percepción por Tipo de Usuario

- **Desarrolladores/Usuarios Técnicos:** Los desarrolladores en Reddit han alabado cada vez más a Claude, especialmente Claude 2 / Claude 3.5, para tareas de codificación. El cambio de percepción a finales de 2024 fue notable: muchos desarrolladores comenzaron a preferir a Claude sobre ChatGPT para asistencia de programación. Citan un rendimiento *“asombroso en codificación”* y la capacidad de manejar bases de código más grandes de una sola vez. Por ejemplo, un usuario escribió *“Claude Sonnet 3.5 es mejor para trabajar con código (analizar, generar) [que ChatGPT].”* Los desarrolladores aprecian que Claude pueda tomar un gran fragmento de código de proyecto o registros y producir análisis o mejoras coherentes, gracias a su enorme contexto. Sin embargo, también notan sus peculiaridades – como a veces inyectar más relleno conversacional o no seguir una especificación al pie de la letra. En balance, muchos desarrolladores mantienen tanto a ChatGPT como a Claude a mano: uno para lógica rigurosa paso a paso (ChatGPT) y otro para contexto amplio y comprensión empática (Claude). Es revelador que un comentarista dijera *“Si tuviera que elegir uno elegiría a Claude”* después de compararlos diariamente. Esto indica una percepción muy positiva entre los usuarios avanzados, especialmente para casos de uso como lluvia de ideas, revisión de código o sugerencias arquitectónicas. La única queja común de los desarrolladores es alcanzar los límites de uso de Claude cuando intentan presionarlo mucho (por ejemplo, alimentando una indicación de 50K tokens para analizar un repositorio completo). En resumen, los desarrolladores ven a Claude como una herramienta extremadamente poderosa – en algunos casos superior a ChatGPT – limitada solo por la disponibilidad y cierta imprevisibilidad en el formato.

- **Usuarios Casuales/No Técnicos:** Los usuarios casuales que han probado a Claude a menudo comentan lo *amigable y articulado* que es. El estilo de Claude tiende a ser conversacional, educado y detallado. Un nuevo usuario comparándolo con ChatGPT observó que *“Claude es más empático y sigue un tono conversacional... ChatGPT por defecto usa viñetas con demasiada frecuencia”*. Esta calidez similar a la humana hace que Claude sea atractivo para las personas que lo usan para escritura creativa, consejos o simplemente chatear para obtener información. Algunos incluso personifican a Claude como teniendo una “personalidad” que es compasiva. Los usuarios casuales también les gusta que la versión gratuita de Claude permitiera el acceso a un equivalente de inteligencia GPT-4 sin una suscripción (al menos hasta los límites de tasa). Por otro lado, los usuarios casuales se topan con los rechazos de Claude en ciertos temas y podrían no entender por qué (ya que Claude lo expresará de manera apologética pero firme). Si un usuario casual preguntó algo límite y recibió un rechazo de Claude, podrían percibirlo como menos capaz o demasiado restringido, sin darse cuenta de que es una postura política. Otro aspecto es que Claude carece del reconocimiento de nombre – muchos usuarios casuales podrían no saber siquiera que deben probarlo a menos que estén conectados a comunidades de IA. Aquellos que lo prueban generalmente comentan que se siente *“como hablar con un humano”* en el buen sentido. Tienden a estar muy satisfechos con la capacidad de Claude para manejar preguntas abiertas o personales. Entonces, la percepción del usuario casual es en gran medida positiva respecto a la *calidad y tono de salida* de Claude, con cierta confusión o frustración en torno a su disponibilidad (tener que usarlo en una aplicación específica o región) y momentos ocasionales de “no puedo hacer eso”.

- **Usuarios Empresariales/Profesionales:** Las percepciones empresariales de Claude son un poco más difíciles de medir a partir de Reddit público (ya que menos usuarios empresariales publican en detalle), pero surgen algunas tendencias. Primero, Anthropic ha posicionado a Claude como más *enfocado en la privacidad* y dispuesto a firmar acuerdos empresariales – esto atrae a empresas preocupadas por los datos con OpenAI. De hecho, algunas discusiones en Reddit mencionan a Claude en el contexto de herramientas como Slack o Notion, donde está integrado como asistente. Los profesionales que han usado esas integraciones podrían no darse cuenta siquiera de que Claude es el motor, pero cuando lo hacen, lo comparan favorablemente en términos de estilo de escritura y la capacidad de digerir grandes documentos corporativos. Por ejemplo, un equipo podría alimentar un largo informe trimestral a Claude y obtener un resumen decente – algo con lo que el contexto más pequeño de ChatGPT lucharía. Dicho esto, los usuarios empresariales también notan la falta de ciertas características del ecosistema; por ejemplo, OpenAI ofrece control de mensajes del sistema, llamadas de funciones, etc., en su API, que Anthropic tiene un soporte más limitado. Un desarrollador trabajando en una solución empresarial comentó que *Claude es más manejable en conversaciones, mientras que ChatGPT tiende a ser más rígido... [pero] ChatGPT tiene acceso web que puede ser muy útil*. La implicación es que para tareas de investigación o búsqueda de datos que un usuario empresarial podría necesitar (como inteligencia competitiva), ChatGPT puede obtener información directamente, mientras que Claude requeriría un paso separado. En general, los usuarios empresariales parecen ver a Claude como una IA muy competente – en algunos casos *mejor* para tareas analíticas internas – pero quizás no tan rica en características aún para la integración. El costo es otro factor: el precio y los términos de la API de Claude no son tan públicos como los de OpenAI, y algunas startups en Reddit han mencionado incertidumbre sobre el precio o estabilidad de Claude. En resumen, los profesionales respetan las capacidades de Claude (especialmente su fiabilidad en seguir instrucciones de alto nivel y resumir entradas grandes), pero observan cómo evoluciona en términos de integración, soporte y disponibilidad global antes de comprometerse completamente con él sobre el más establecido ChatGPT.

---

## Google Gemini (Bard)

### Puntos de Dolor Comunes y Limitaciones

- **Respuestas inexactas o “tontas”:** Una avalancha de comentarios en Reddit apareció cuando Google lanzó su actualización de Bard impulsada por Gemini, gran parte de ella negativa. Los usuarios se quejaron de que Gemini **rindió por debajo en QA básico** en comparación con ChatGPT. Una evaluación contundente titulada “100% Opinión Honesta sobre Google Gemini” declaró: *“Es un chatbot LLM roto e inexacto”*. Otro usuario frustrado preguntó: *“¿Cómo es que Gemini sigue siendo tan malo? La cantidad de veces que le pido algo a Gemini y me da respuestas incorrectas o incompletas es ridícula”*. Lo compararon lado a lado con ChatGPT-4 y encontraron que ChatGPT dio *“una respuesta perfecta, correcta y eficiente de una sola vez,”* mientras que Gemini divagaba y requería múltiples indicaciones para llegar a una respuesta medio satisfactoria. En esencia, los primeros usuarios sintieron que Gemini frecuentemente **alucinaba o perdía el punto** de las preguntas, requiriendo un esfuerzo excesivo de indicaciones para extraer información correcta. Esta inconsistencia en la calidad fue una gran decepción dado el bombo alrededor de Gemini.

- **Verborragia y relleno excesivos:** Muchos usuarios notaron que Gemini (en forma del nuevo Bard) tiende a producir respuestas largas que no van al grano. Como describió una persona, *“Divagó... 3 párrafos de basura de IA... incluso entonces, [solo] eventualmente mencionó la respuesta enterrada en párrafos de basura”*. Esto contrasta marcadamente con ChatGPT, que a menudo ofrece respuestas más concisas o viñetas cuando es apropiado. La verborragia se convierte en un punto de dolor cuando los usuarios tienen que tamizar mucho texto para obtener un simple hecho. Algunos especularon que Google podría haberlo ajustado para ser conversacional o “útil,” pero se pasó de la raya en *demasiada* explicación sin sustancia.

- **Integración deficiente con los propios servicios de Google:** Uno de los puntos de venta del asistente de IA de Google se supone que es la integración con el ecosistema de Google (Gmail, Docs, Drive, etc.). Sin embargo, las primeras experiencias de los usuarios fueron muy decepcionantes en este frente. Un usuario se desahogó: *“Ni siquiera me hagas empezar con su casi total incapacidad para integrarse con los propios productos de Google, lo cual se supone que es una ‘característica’ (que aparentemente no sabe que tiene).”*. Por ejemplo, las personas intentaban pedirle a Gemini (a través de Bard) que resumiera un Google Doc o redactara un correo electrónico basado en alguna información – características que Google anunció – y el bot respondía que **no puede acceder a esos datos**. Un usuario en r/GooglePixel escribió: *“Cada vez que intento usar Gemini con mis Google Docs o Drive, me dice que no puede hacer nada con ello. ¿Cuál es el punto de tener siquiera estas características de integración?”*. Esto muestra una brecha significativa entre las capacidades prometidas y el rendimiento real, dejando a los usuarios sintiendo que el “asistente de IA” no está ayudando mucho dentro del propio ecosistema de Google.

- **Rechazos y confusión de capacidades:** Los usuarios también encontraron rechazos o contradicciones extrañas de Gemini. El mismo Redditor notó que Gemini *“se niega a hacer cosas sin razón, olvida que puede hacer otras cosas... El otro día me dijo que no tenía acceso a internet/datos en vivo. ¿Qué?”*. Esto indica que Gemini a veces **declina tareas que debería poder hacer** (como recuperar información en vivo, a la que Bard está conectado) o hace declaraciones incorrectas sobre sus propias habilidades. Tales experiencias dieron la impresión de una IA que no solo es menos inteligente, sino también **menos confiable o consciente de sí misma**. Otro comentario colorido de un usuario: *“Gemini es una basura absoluta. ¿Alguna vez has tenido uno de esos momentos en los que solo quieres levantar las manos y decir, ‘¿En qué estaban pensando?’”* encapsula la frustración. Esencialmente, los problemas de integración y consistencia del producto de Gemini hicieron que se sintiera *a medio hacer* para muchos primeros usuarios.

- **Habilidades de codificación poco destacables:** Aunque no se discute tan ampliamente como el QA general, varios usuarios probaron a Gemini (Bard) en tareas de codificación y lo encontraron deficiente. En foros de IA, las capacidades de codificación de Gemini generalmente se calificaron por debajo de GPT-4 e incluso por debajo de Claude. Por ejemplo, un usuario declaró claramente que *“Claude 3.5 Sonnet es claramente mejor para codificar que ChatGPT 4o... Gemini es una basura absoluta [en ese contexto]”*. El consenso fue que Gemini podía escribir código simple o explicar algoritmos básicos, pero a menudo tropezaba con problemas más complejos o producía código con errores. Su falta de un conjunto de herramientas de desarrollador amplio (por ejemplo, no tiene un equivalente de Code Interpreter o llamadas de funciones robustas) también significaba que no era una primera opción para programadores. Entonces, aunque no todos los usuarios casuales se preocupan por el código, esta es una limitación para ese segmento.

- **Limitaciones en dispositivos móviles:** Gemini se lanzó como parte del Asistente de Google en teléfonos Pixel (marcado como “Asistente con Bard”). Algunos usuarios de Pixel notaron que usarlo como reemplazo del asistente de voz tenía problemas. A veces no captaba con precisión las indicaciones de voz o tardaba demasiado en responder en comparación con el antiguo Asistente de Google. También hubo comentarios sobre la necesidad de optar por participar y perder algunas características clásicas del Asistente. Esto creó la percepción de que *la integración de Gemini en dispositivos no estaba completamente lista*, dejando a los usuarios avanzados del ecosistema de Google sintiendo que tenían que elegir entre un asistente inteligente y uno funcional.

### Características o Mejoras Solicitadas Frecuentemente

- **Mejora drástica de la precisión y el razonamiento:** La mejora número uno que los usuarios quieren para Gemini es simplemente **ser más inteligente y confiable**. Los comentarios en Reddit dejan claro que Google necesita cerrar la brecha en la calidad de las respuestas. Los usuarios esperan que Gemini utilice el vasto acceso a la información de Google para dar *respuestas factuales y directas*, no divagaciones o incorrectas. Entonces, las solicitudes (a menudo formuladas sarcásticamente) se reducen a: *hazlo tan bueno como o mejor que GPT-4 en conocimiento general y razonamiento.* Esto incluye un mejor manejo de preguntas de seguimiento e indicaciones complejas. Esencialmente, “arregla el cerebro” de Gemini – aprovecha esas supuestas ventajas de entrenamiento multimodal para que deje de perder detalles obvios. Google probablemente ha escuchado esto alto y claro: muchas publicaciones comparan respuestas específicas donde ChatGPT sobresalió y Gemini falló, lo que sirve como informes de errores informales para la mejora.

- **Mejor integración y conciencia de contexto:** Los usuarios quieren que Gemini cumpla la promesa de un ayudante de ecosistema de Google sin fisuras. Esto significa que debería **interactuar adecuadamente con Gmail, Calendar, Docs, Drive, etc.** Si un usuario pide “Resumir el documento que abrí” o “Redactar una respuesta al último correo de mi jefe,” la IA debería hacerlo – y hacerlo de manera segura. En este momento, la solicitud es que Google *habilite esas características y haga que Gemini realmente reconozca cuando tal tarea es posible*. Se anunció que Bard podría conectarse al contenido del usuario (con permiso), por lo que los usuarios están efectivamente exigiendo que Google “encienda” o arregle esta integración. Esta es una característica clave especialmente para usuarios empresariales. Además, en el frente de navegación web: Bard (Gemini) puede buscar en la web, pero algunos usuarios quieren que cite fuentes más claramente o sea más oportuno al incorporar noticias de última hora. Así que mejorar la naturaleza *conectada* de Gemini es una solicitud frecuente.

- **Controles de concisión:** Dadas las quejas de verborragia, algunos usuarios sugieren una característica para alternar el estilo de respuesta. Por ejemplo, un *“modo breve”* donde Gemini da una respuesta corta y directa por defecto, a menos que se le pida que elabore. Por el contrario, tal vez un “modo detallado” para aquellos que quieren respuestas muy completas. ChatGPT permite implícitamente algo de esto mediante la indicación del usuario (“manténlo breve”); con Gemini, los usuarios sintieron que incluso cuando no pedían detalles, sobreexplicaba. Así que una configuración incorporada o simplemente un mejor ajuste para producir respuestas concisas cuando sea apropiado sería una mejora bienvenida. En esencia, ajustar el dial de verborragia.

- **Paridad de características con ChatGPT (codificación, complementos, etc.):** Los usuarios avanzados en Reddit comparan explícitamente características. Solicitan que Gemini/Bard de Google ofrezca cosas como un *sandbox de ejecución de código* (similar al Code Interpreter de ChatGPT), la capacidad de cargar imágenes/PDFs para análisis (dado que Gemini es multimodal, los usuarios quieren realmente alimentarlo con imágenes personalizadas, no solo que describa las proporcionadas). Otra característica mencionada frecuentemente es una mejor **memoria dentro de la conversación** – aunque Bard tiene algo de memoria de interacciones pasadas, los usuarios quieren que sea tan bueno como ChatGPT en referenciar contexto anterior, o incluso tener almacenamiento persistente de conversaciones como el historial de chat de ChatGPT que puedes desplazarte y revisar. Esencialmente, se le pide a Google que se ponga al día en todas las características de calidad de vida que los usuarios de ChatGPT Plus tienen: historial de chat, ecosistema de complementos (o al menos integraciones sólidas de terceros), asistencia de codificación, etc.

- **Mejoras en la aplicación móvil y el asistente de voz:** Muchos usuarios casuales solicitaron una **aplicación móvil dedicada para Bard/Gemini** (similar a la aplicación móvil de ChatGPT). Confiar en una interfaz web o solo el Asistente de Pixel es limitante. Una aplicación oficial en iOS/Android con entrada de voz, respuestas habladas (para una verdadera sensación de asistente) e integración estrecha podría mejorar enormemente la experiencia del usuario. Junto con eso, los propietarios de Pixel quieren que el Asistente con Bard sea más rápido y funcional – básicamente, quieren lo mejor del antiguo Asistente de Google (acciones rápidas y precisas) combinado con la inteligencia de Gemini. Por ejemplo, cosas como continuar permitiendo comandos de voz “Hey Google” para el hogar inteligente y no solo respuestas conversacionales. Google podría mejorar el modo de voz de Gemini para reemplazar verdaderamente al asistente heredado sin regresiones de características.

- **Transparencia y control:** Algunos usuarios han pedido más información sobre las fuentes de Bard o una forma de ajustar su estilo. Por ejemplo, mostrar de qué resultado de Google Bard está extrayendo información (para verificar la precisión) – algo que Bing Chat hace citando enlaces. Además, debido a que Bard ocasionalmente produce información incorrecta, los usuarios quieren poder marcarla o corregirla, e idealmente Bard debería aprender de esa retroalimentación con el tiempo. Tener un mecanismo de retroalimentación fácil (“pulgar hacia abajo – esto es incorrecto porque...”) que conduzca a una mejora rápida del modelo infundiría confianza de que Google está escuchando. Básicamente, características para hacer que la IA sea más un asistente colaborativo que una caja negra.

### Necesidades Desatendidas o Segmentos de Usuarios

- **Usuarios que buscan un asistente personal confiable:** Irónicamente, el grupo que Google *apuntó* – personas que quieren un asistente personal poderoso – se sienten más desatendidos por Gemini en su forma actual. Los primeros adoptantes que activaron el nuevo Asistente basado en Bard esperaban una actualización, pero muchos sintieron que era una degradación en términos prácticos. Por ejemplo, si alguien quiere un asistente de voz para *responder con precisión* trivia, establecer recordatorios, controlar dispositivos e integrar información de sus cuentas, Gemini tuvo problemas. Esto dejó al segmento mismo de profesionales ocupados o entusiastas de gadgets (que dependen de asistentes para la productividad) sintiendo que sus necesidades no se cumplieron. Un usuario comentó que consideraría pagar por el “Asistente con Bard” de Pixel *“si [supera] al Asistente de Google”*, implicando que aún no lo había hecho. Así que ese segmento todavía está esperando un asistente de IA confiable y realmente útil – se lanzarán sobre él si Gemini mejora.

- **Hablantes no nativos de inglés / localización:** Los productos de Google generalmente tienen una excelente localización, pero no está claro si Bard/Gemini fue igualmente fuerte en todos los idiomas al lanzamiento. Algunos usuarios internacionales informaron que las respuestas de Bard en su idioma nativo eran menos fluidas o útiles, empujándolos de nuevo a competidores locales. Si los datos de entrenamiento o la optimización de Gemini favorecieron el inglés, entonces los usuarios no ingleses están desatendidos. Podrían preferir ChatGPT o modelos locales que han optimizado explícitamente capacidades multilingües. Este es un espacio en el que Google podría tradicionalmente sobresalir (dado su tecnología de traducción), pero los comentarios de los usuarios sobre eso son escasos – probablemente indicando que Gemini aún no ha impresionado a esas comunidades.

- **Clientes empresariales (hasta ahora):** Las grandes organizaciones no han adoptado ampliamente Bard/Gemini según las conversaciones públicas, a menudo debido a brechas de confianza y capacidad. Las empresas necesitan consistencia, citas e integración con sus flujos de trabajo (Office 365 está profundamente integrado con la tecnología de OpenAI a través de MS Copilot, por ejemplo). El equivalente de Google (Duet AI con Gemini) aún está evolucionando. Hasta que Gemini/Bard demuestre que puede redactar correos electrónicos de manera confiable, crear presentaciones de diapositivas o analizar datos en Google Sheets a un nivel a la par o superior a GPT-4, los usuarios empresariales sentirán que la solución de Google no está abordando completamente sus necesidades. Algunas publicaciones en r/Bard de profesionales son del tipo “Probé Bard para tareas de trabajo, no fue tan bueno como ChatGPT, así que esperaremos y veremos.” Eso indica que los usuarios empresariales son un segmento desatendido por ahora – quieren una IA que se integre en Google Workspace y realmente aumente la productividad sin necesidad de verificación constante de salidas.

- **Usuarios en el ecosistema de Google que prefieren soluciones todo en uno:** Hay un segmento de usuarios que usan Google para todo (búsqueda, correo electrónico, documentos) y *felizmente* usarían una IA de Google para todas sus necesidades de chatbot – si fuera tan buena. En este momento, esos usuarios están algo desatendidos porque terminan usando ChatGPT para ciertas cosas y Bard para otras. Podrían hacer preguntas factuales a ChatGPT porque confían más en su calidad de respuesta, pero usar Bard para sus intentos de integración o navegación. Esa experiencia dividida no es ideal. Tales usuarios realmente solo quieren quedarse en una aplicación/asistente. Si Gemini mejora, se consolidarán a su alrededor, pero hasta entonces su caso de uso de “un asistente para gobernarlos a todos” no está cumplido.

- **Desarrolladores/Científicos de datos en Google Cloud:** Google lanzó modelos Gemini a través de su plataforma Vertex AI para desarrolladores. Sin embargo, los primeros informes y puntos de referencia sugirieron que Gemini (particularmente el modelo “Gemini Pro” disponible) no estaba superando a GPT-4. Los desarrolladores que prefieren Google Cloud para servicios de IA son así un poco desatendidos por la calidad del modelo – tienen que aceptar un modelo ligeramente inferior o integrar la API de OpenAI por separado. Este segmento de desarrolladores empresariales está hambriento de un modelo fuerte de Google para poder mantener todo en una sola pila. Hasta que el rendimiento de Gemini se destaque claramente en algunas áreas o el precio ofrezca una razón convincente, no está sirviendo completamente las necesidades de este grupo en términos competitivos.

### Diferencias en la Percepción por Tipo de Usuario

- **Desarrolladores/Entusiastas de la Tecnología:** Los usuarios conocedores de tecnología se acercaron a Gemini con altas expectativas (es Google, después de todo). Su percepción se agrió rápidamente después de pruebas prácticas. Muchos desarrolladores en Reddit realizaron puntos de referencia o sus preguntas difíciles favoritas a través de Gemini y lo encontraron rezagado. Un programador declaró sin rodeos, *“Gemini es una basura absoluta como Llama 3.0 solía ser”*, indicando que lo clasifican incluso por debajo de algunos modelos abiertos. Los desarrolladores son particularmente sensibles a los errores lógicos y la verborragia. Así que cuando Gemini dio respuestas verbosas pero incorrectas, perdió credibilidad rápidamente. Por otro lado, los desarrolladores reconocen el potencial de Google; algunos mantienen la esperanza de que *“con más ajuste fino, Gemini mejorará”* y lo vuelven a probar periódicamente después de actualizaciones. En el presente, sin embargo, la mayoría de los desarrolladores perciben que es **inferior a GPT-4** en casi todas las tareas serias (codificación, resolución de problemas complejos). Aprecian ciertas cosas: por ejemplo, Gemini tiene acceso a información en tiempo real (a través de la búsqueda de Google) sin necesidad de un complemento, lo cual es útil para consultas actualizadas. Un desarrollador podría usar Bard para algo como “buscar y resumir los últimos artículos sobre X,” donde puede citar datos web. Pero para razonamiento autónomo, se inclinan hacia otros modelos. En resumen, los entusiastas de la tecnología ven a Gemini como un trabajo en progreso prometedor que *actualmente* se siente una generación atrás. No ha ganado su plena confianza, y a menudo publican comparaciones lado a lado destacando sus errores para impulsar a Google a mejorarlo.

- **Usuarios Casuales/Cotidianos:** Los usuarios casuales, incluidos aquellos que obtuvieron acceso al nuevo Bard en sus teléfonos o a través de la web, tuvieron sentimientos encontrados. Muchos usuarios casuales inicialmente se acercaron a Bard (Gemini) porque es gratuito y fácil de acceder con una cuenta de Google, a diferencia de GPT-4 que estaba detrás de un muro de pago. Algunos usuarios casuales realmente informan experiencias decentes para usos simples: por ejemplo, un Redditor en r/Bard dio una revisión positiva señalando que Gemini les ayudó con cosas como revisar documentos legales, redacción publicitaria e incluso un caso de uso divertido de identificar tallas de ropa a partir de una foto. Dijeron *“Gemini ha sido un recurso valioso para responder mis preguntas... información actualizada... Me he acostumbrado tanto a la versión de pago que no puedo recordar cómo funciona la versión gratuita.”* – indicando que al menos *algunos* usuarios casuales que invirtieron tiempo (y dinero) en Bard Advanced lo encontraron útil en la vida diaria. Estos usuarios tienden a usarlo para ayuda práctica y cotidiana y pueden no llevar el modelo a sus límites. Sin embargo, muchos otros usuarios casuales (especialmente aquellos que también habían probado ChatGPT) se sintieron decepcionados. Las personas comunes que piden cosas como consejos de viaje, trivia o ayuda con una tarea encontraron que las respuestas de Bard eran menos claras o útiles. La percepción aquí está dividida: **usuarios leales a la marca Google** vs. **aquellos ya malcriados por ChatGPT**. El primer grupo, si no habían usado mucho ChatGPT, a veces encuentran que Bard/Gemini es “bastante bueno” para sus necesidades y aprecian que esté integrado con la búsqueda y sea gratuito. El segundo grupo casi invariablemente compara y encuentra a Gemini deficiente. Podrían decir, *“¿Por qué usaría Bard cuando ChatGPT es mejor el 90% del tiempo?”*. Así que la percepción del usuario casual realmente depende de su marco de referencia previo. Aquellos nuevos en asistentes de IA podrían calificar a Gemini como una novedad útil; aquellos experimentados con la competencia lo ven como una decepción que *“todavía apesta tanto”* y necesita mejorar.

- **Usuarios Empresariales/Profesionales:** Muchos profesionales le dieron una oportunidad a Bard cuando se lanzó con integración en Google Workspace (Duet AI). La percepción entre este grupo es de escepticismo cauteloso. Por un lado, confían en las promesas empresariales de Google respecto a la privacidad de datos e integración (por ejemplo, edición de Docs a través de IA, resúmenes de reuniones a partir de invitaciones de Calendar, etc.). Por otro lado, las primeras pruebas a menudo mostraron que Gemini cometía errores factuales o proporcionaba salidas genéricas, lo cual no inspira confianza para el uso empresarial. Por ejemplo, un profesional podría pedirle a Bard que redacte un informe para un cliente – si Bard inserta datos incorrectos o ideas débiles, podría ser más problemático que útil. Por lo tanto, los usuarios profesionales tienden a *pilotar* Bard en tareas no críticas pero aún se apoyan en GPT-4 o Claude para salidas importantes. También hay una percepción de que Google estaba jugando a ponerse al día: muchos vieron a Bard como “no listo para el horario estelar” y decidieron esperar. Existe una percepción positiva en áreas como **consultas de datos en tiempo real** – por ejemplo, un analista financiero en Reddit señaló que Bard podría obtener información reciente del mercado gracias a la búsqueda de Google, lo que ChatGPT no podría a menos que los complementos estuvieran habilitados. Así que en dominios donde los datos actuales son clave, algunos profesionales vieron una ventaja. Otro matiz: las personas en el ecosistema de Google (por ejemplo, empresas que usan exclusivamente Google Workspace) tienen una visión ligeramente más favorable simplemente porque Bard/Gemini es la opción que se adapta a su entorno. Están apoyando que mejore en lugar de cambiar a un ecosistema completamente diferente. En resumen, los usuarios empresariales ven a Gemini como *potencialmente muy útil* (dado los datos y la integración de herramientas de Google), pero a principios de 2025, aún no ha ganado plena confianza. Lo perciben como el “nuevo contendiente que aún no está del todo listo” – vale la pena monitorear, pero aún no es una opción preferida para tareas críticas. La reputación de Google le compra algo de paciencia a esta multitud, pero no indefinida; si Gemini no mejora notablemente, los profesionales podrían no adoptarlo ampliamente, quedándose con otras soluciones.

---

## LLMs de Código Abierto (por ejemplo, Modelos Basados en LLaMA)

### Puntos de Dolor Comunes y Limitaciones

- **Requisitos de hardware y configuración:** A diferencia de los chatbots en la nube, los LLMs de código abierto generalmente requieren que los usuarios los ejecuten en hardware local o un servidor. Esto presenta inmediatamente un punto de dolor: muchos modelos (por ejemplo, un modelo LLaMA de 70 mil millones de parámetros) necesitan una GPU poderosa con mucha VRAM para funcionar sin problemas. Como lo expresó sucintamente un Redditor, *“Los LLMs locales en la mayoría del hardware de consumo no van a tener la precisión necesaria para ningún desarrollo complejo.”* Para la persona promedio con solo una GPU de 8GB o 16GB (o solo una CPU), ejecutar un modelo de alta calidad puede ser lento o directamente inviable. Los usuarios podrían recurrir a modelos más pequeños que se ajusten, pero esos a menudo producen salidas de menor calidad (respuestas “más tontas”). La complejidad de la configuración es otro problema – instalar pesos de modelo, configurar entornos como Oobabooga o LangChain, gestionar bibliotecas de tokenización, etc., puede ser intimidante para los no desarrolladores. Incluso los usuarios técnicamente capacitados lo describen como una molestia para mantenerse al día con nuevas versiones de modelos, peculiaridades de controladores de GPU, y así sucesivamente. Un hilo titulado “En serio, ¿cómo usas realmente los LLMs locales?” tenía personas compartiendo que muchos modelos *“o rinden por debajo o no funcionan sin problemas en mi hardware”*, y pidiendo consejos prácticos.

- **Rendimiento inferior a los modelos cerrados de última generación:** Los modelos de código abierto han progresado rápidamente, pero a partir de 2025 muchos usuarios notan que aún están rezagados respecto a los modelos propietarios de primera línea (GPT-4, Claude) en razonamiento complejo, codificación y precisión factual. Un ejemplo vívido: un usuario en r/LocalLLaMA comparó salidas en su idioma nativo y dijo *“Todos los demás modelos que he probado fallan... No se acercan [a GPT-4]. ChatGPT 4 es absolutamente asombroso escribiendo”*. Este sentimiento se repite ampliamente: mientras que los modelos abiertos más pequeños (como un 13B o 7B ajustado) pueden ser impresionantes para su tamaño, luchan con tareas que requieren comprensión profunda o lógica de varios pasos. Incluso los modelos abiertos más grandes (65B, 70B) que se acercan al nivel de GPT-3.5 aún pueden fallar en el tipo de problemas complicados que maneja GPT-4. Los usuarios observan más alucinaciones y errores en modelos abiertos, especialmente en conocimiento de nicho o cuando las indicaciones se desvían ligeramente de la distribución de entrenamiento. Entonces, la brecha en capacidad bruta es un punto de dolor – uno debe moderar las expectativas al usar modelos locales, lo que puede ser frustrante para aquellos acostumbrados a la fiabilidad de ChatGPT.

- **Límite de longitud de contexto:** La mayoría de los LLMs de código abierto tradicionalmente tienen ventanas de contexto más pequeñas (2048 tokens, tal vez 4k tokens) en comparación con lo que ofrecen ChatGPT o Claude. Algunos ajustes y arquitecturas más nuevos están extendiendo esto (por ejemplo, hay versiones de 8K o 16K tokens de LLaMA-2, y la investigación como MPT-7B tenía un contexto de 16K). Sin embargo, el uso práctico de modelos abiertos de contexto muy largo aún está en etapas tempranas. Esto significa que los usuarios de modelos locales enfrentan problemas de memoria similares – el modelo olvida partes anteriores de la conversación o texto, a menos que implementen esquemas de memoria externa (como bases de datos vectoriales para recuperación). En discusiones de Reddit, los usuarios a menudo mencionan tener que resumir o truncar manualmente el historial para mantenerse dentro de los límites, lo cual es laborioso. Esta es una limitación notable especialmente ya que los modelos propietarios están empujando las longitudes de contexto más allá (como los 100k de Claude).

- **Falta de ajuste fino de seguimiento de instrucciones en algunos modelos:** Aunque muchos modelos abiertos están ajustados por instrucciones (Alpaca, LLaMA-2-Chat, etc.), no todos están tan rigurosamente entrenados con RLHF como ChatGPT. Esto puede resultar en que los modelos locales a veces sean menos receptivos a instrucciones o indicaciones del sistema. Por ejemplo, un modelo LLaMA en bruto simplemente continuará texto e ignorará completamente un formato de indicación de usuario – uno debe usar una versión ajustada para chat. Incluso entonces, la calidad de los datos de ajuste importa. Algunos usuarios de Reddit notaron que ciertos modelos de instrucciones ya sea *rechazaron* excesivamente (porque fueron ajustados con seguridad pesada, por ejemplo, algunos chats de LLaMA-2 de Facebook responderían con rechazos de política similares a los de ChatGPT) o *rindieron por debajo* (no siguiendo la consulta con precisión). Una queja de un usuario en GitHub sobre CodeLlama-70B-instruct dijo que *“está tan censurado que es básicamente inútil”*, mostrando frustración de que un modelo abierto adoptara la misma estrictitud sin la alternativa de desactivarla. Entonces, dependiendo del modelo elegido, los usuarios podrían enfrentar un modelo que es demasiado suelto (y da continuación irrelevante) o uno que es demasiado estricto/guardado. Obtener un comportamiento de seguimiento de instrucciones bien equilibrado a menudo requiere probar múltiples ajustes.

- **Fragmentación y cambio rápido:** El panorama de LLMs de código abierto evoluciona extremadamente rápido, con nuevos modelos y técnicas (cuantización, ajustes LoRA, etc.) emergiendo semanalmente. Aunque emocionante, esto es un punto de dolor para los usuarios que no quieren ajustar constantemente su configuración. Lo que funcionó el mes pasado podría estar desactualizado este mes. Un Redditor humorosamente lo comparó con el salvaje oeste, diciendo que la comunidad está *“encontrando formas de ‘fingirlo’ para que se sienta como si fuera similar [a GPT-4]”* pero a menudo estas son soluciones temporales. Para un usuario casual, es desalentador incluso elegir entre docenas de nombres de modelos (Vicuna, Alpaca, Mythomax, Mistral, etc.), cada uno con múltiples versiones y bifurcaciones. Sin una plataforma unificada, los usuarios dependen de guías comunitarias – que pueden ser confusas – para decidir qué modelo se adapta a sus necesidades. Esta fragmentación en herramientas y calidad de modelos es un punto de dolor indirecto: eleva la barrera de entrada y el esfuerzo de mantenimiento.

- **Sin soporte oficial o garantías:** Cuando algo sale mal con un LLM local (por ejemplo, el modelo produce contenido ofensivo o se bloquea), no hay soporte al cliente a quien llamar. Los usuarios están por su cuenta o dependen de la ayuda comunitaria. Para los aficionados esto está bien, pero para el uso profesional esta falta de soporte formal es una barrera. Algunos usuarios de Reddit que trabajan en empresas notaron que aunque les encantaría la privacidad de un modelo abierto, les preocupa a quién acudir si el modelo falla o si necesitan actualizaciones. Esencialmente, usar código abierto es DIY – tanto una fortaleza como una debilidad.

### Características o Mejoras Solicitadas Frecuentemente

- **Mejor eficiencia (cuantización y optimización):** Un enfoque importante en la comunidad (y por lo tanto una solicitud común) es hacer que los modelos grandes funcionen en hardware más pequeño. Los usuarios esperan con ansias técnicas que permitan que un modelo de 70B funcione tan suavemente como un modelo de 7B. Ya hay cuantización de 4 bits o 8 bits, y los hilos a menudo discuten nuevos métodos como AWQ o adaptadores similares a RNN. Un usuario citó investigaciones donde la cuantización mejorada podría mantener la calidad a menor precisión de bits. El deseo es esencialmente: *“Déjame ejecutar un modelo al nivel de GPT-4 en mi PC sin retraso.”* Cada avance que se acerque (como arquitecturas de transformadores más eficientes o descarga de GPU a CPU) es celebrado. Así que las solicitudes de mejores herramientas (como la próxima generación de llama.cpp u otros aceleradores) son comunes – cualquier cosa para reducir la barrera de hardware.

- **Modelos más grandes y mejores (cerrando la brecha de calidad):** La comunidad empuja constantemente por nuevos modelos abiertos de última generación. Los usuarios están emocionados por proyectos como LLaMA 3 (si/cuando Meta lanza uno) o colaboraciones que podrían producir un modelo abierto de 100B+. Muchos expresan optimismo de que *“tendremos modelos GPT-4 locales en nuestras máquinas para fin de año”*. En esa cita, el usuario apuesta a que LLaMA 3 más ajuste fino entregará un rendimiento similar a GPT-4. Así que, se podría decir que una “característica solicitada” es simplemente: **más peso, más entrenamiento** – la comunidad quiere que las empresas tecnológicas o grupos de investigación abran modelos más grandes y mejores para que puedan ejecutarlos localmente. Cada vez que sale un nuevo modelo (como Mistral 7B o Falcon 40B), los usuarios prueban si supera al anterior. La solicitud final es un modelo abierto que realmente rivalice con GPT-4, eliminando la necesidad de IA cerrada para aquellos que pueden alojarlo.

- **Interfaces amigables para el usuario y configuraciones de un clic:** Para ampliar la adopción, muchos usuarios piden formas más fáciles de usar LLMs locales. Esto incluye interfaces GUI donde uno puede descargar un modelo y comenzar a chatear sin trabajo de línea de comandos. Hay proyectos que abordan esto (text-generation-webui de Oobabooga, LM Studio, etc.), pero los recién llegados aún luchan. Un hilo reciente de Reddit podría preguntar, *“¿Cómo configuro un LLM similar a ChatGPT localmente?”*, con usuarios solicitando guías paso a paso. Así que un deseo frecuente es una **instalación simplificada** – tal vez una aplicación oficial o contenedor Docker que agrupe todo lo necesario, o integración en software popular (imagina una extensión que lleve un LLM local a VSCode o Chrome fácilmente). Esencialmente, reducir la sobrecarga técnica para que las personas menos expertas en tecnología también puedan disfrutar de LLMs privados.

- **Contexto más largo y memoria para modelos locales:** Los desarrolladores y usuarios de código abierto están experimentando con extender el contexto (a través de ajustes de incrustación posicional o modelos especializados). Muchos usuarios solicitan que los nuevos modelos vengan con ventanas de contexto más largas por defecto – por ejemplo, un modelo abierto con contexto de 32k sería muy atractivo. Hasta que eso suceda, algunos dependen de soluciones de “recuperación” externas (LangChain con una tienda vectorial que alimenta información relevante en la indicación). Los usuarios en r/LocalLLaMA discuten frecuentemente sus configuraciones para pseudo-contexto largo, pero también expresan el deseo de que los modelos mismos manejen más. Así que una mejora que buscan es: *“Danos un Claude local – algo con decenas de miles de tokens de contexto.”* Esto les permitiría hacer análisis de libros, conversaciones largas o trabajo de grandes bases de código localmente.

- **Herramientas de ajuste fino mejoradas y personalización de modelos:** Otra solicitud es hacer más fácil ajustar fino o personalizar modelos. Aunque existen bibliotecas para ajustar modelos en nuevos datos (Alpaca lo hizo con 52K instrucciones, Low-Rank Adaptation (LoRA) permite ajuste fino con computación limitada, etc.), aún es algo complicado. A los usuarios les encantaría más herramientas accesibles para, por ejemplo, alimentar todos sus escritos o documentos de la empresa al modelo y que se adapte. Proyectos como LoRA son pasos en esa dirección, pero una solución más automatizada (tal vez una interfaz de asistente: “sube tus documentos aquí para ajustar fino”) sería bienvenida. Esencialmente, llevar la capacidad que OpenAI proporciona a través de la API (ajuste fino de modelos en datos personalizados) al ámbito local de manera amigable para el usuario.

- **Herramientas de seguridad y moderación impulsadas por la comunidad:** Dado que los modelos abiertos pueden producir cualquier cosa (incluido contenido no permitido), algunos usuarios han solicitado o comenzado a desarrollar capas de moderación que los usuarios pueden alternar o ajustar. Esto es un poco nicho, pero la idea es tener filtros *opcionales* para capturar salidas atroces si alguien los quiere (por ejemplo, si niños o estudiantes podrían interactuar con el modelo localmente). Dado que los modelos abiertos no se detendrán por sí mismos, tener un complemento o script para escanear salidas en busca de contenido extremo podría ser útil. Algunos en la comunidad trabajan en “guardarraíles éticos” a los que puedes optar, lo cual es interesante porque da control al usuario. Así que, características en torno a **controlar el comportamiento del modelo** – ya sea para hacerlo más seguro o para eliminar seguridades – se discuten y solicitan a menudo, dependiendo de los objetivos del usuario.

### Necesidades Desatendidas o Segmentos de Usuarios

- **Usuarios no técnicos que valoran la privacidad:** En este momento, los LLMs locales en gran medida atienden a entusiastas de la tecnología. Una persona que no es experta en computadoras pero se preocupa por la privacidad de los datos (por ejemplo, un psicoterapeuta que quiere ayuda de IA analizando notas pero no puede subirlas a la nube) está desatendida. Necesitan una solución local que sea fácil y segura, pero la complejidad es una barrera. Hasta que la IA local se vuelva tan fácil como instalar una aplicación, estos usuarios permanecen al margen – ya sea comprometiéndose al usar IA en la nube y arriesgando la privacidad, o no usando IA en absoluto. Este segmento – individuos conscientes de la privacidad pero no altamente técnicos – está claramente desatendido por las ofertas actuales de código abierto.

- **Usuarios conscientes del presupuesto en regiones con internet deficiente:** Otro segmento que se beneficia de modelos locales son las personas que no tienen internet confiable o no pueden pagar llamadas a la API. Si alguien pudiera obtener un chatbot decente sin conexión en una máquina de gama baja, sería valioso (imagina educadores o estudiantes en áreas remotas). Actualmente, la calidad sin conexión podría no ser excelente a menos que tengas una PC de gama alta. Hay algunos modelos muy pequeños que funcionan en teléfonos, pero su capacidad es limitada. Entonces, los usuarios que *necesitan IA sin conexión* – debido a conectividad o costo – son un grupo que el código abierto podría servir, pero la tecnología está justo en el borde de ser lo suficientemente útil. Estarán mejor servidos a medida que los modelos se vuelvan más eficientes.

- **Creadores de contenido NSFW o especializado:** Una razón por la que los modelos abiertos ganaron popularidad es que pueden ser sin censura, permitiendo casos de uso que las IAs cerradas prohíben (juego de roles erótico, exploración de ficción violenta, etc.). Aunque este segmento “desatendido” es controvertido, es real – muchas comunidades de Reddit (por ejemplo, para AI Dungeon o chatbots de personajes) se trasladaron a modelos locales después de que OpenAI y otros endurecieron las reglas de contenido. Estos usuarios ahora son atendidos por modelos abiertos en cierta medida, pero a menudo tienen que encontrar o ajustar modelos específicamente para este propósito (como Mythomax para narración, etc.). Ocasionalmente lamentan que muchos modelos abiertos aún tienen restos de entrenamiento de seguridad (rechazando ciertas solicitudes). Así que desean modelos explícitamente ajustados para creatividad sin censura. Se podría argumentar que *están* siendo atendidos (ya que tienen soluciones), pero no por los valores predeterminados principales – dependen de bifurcaciones comunitarias de nicho.

- **Comunidades lingüísticas y culturales:** Los modelos de código abierto podrían ajustarse para idiomas específicos o conocimiento local, pero la mayoría de los prominentes son centrados en inglés. Los usuarios de comunidades no inglesas pueden estar desatendidos porque ni OpenAI ni los modelos abiertos atienden perfectamente a su idioma/jerga/contexto cultural. Hay esfuerzos (como BLOOM y variantes XLM) para construir modelos abiertos multilingües, y los usuarios locales solicitan ajustes en idiomas como español, árabe, etc. Si alguien quiere un chatbot profundamente fluido en su dialecto regional o actualizado sobre noticias locales (en su