---
title: "Feedback de Usuários do Reddit sobre Principais Ferramentas de Chat LLM"
tags: [IA, ChatGPT, Claude, Google Gemini, LLMs de Código Aberto]
keywords: [ferramentas de chat IA, feedback de usuários, ChatGPT, Claude, Google Gemini, LLMs de código aberto, análise do Reddit]
authors: [lark]
description: Este artigo fornece uma análise aprofundada das discussões no Reddit sobre ferramentas populares de chat IA, incluindo ChatGPT, Claude, Google Gemini e LLMs de código aberto. Destaca os pontos problemáticos relatados pelos usuários, recursos frequentemente solicitados e necessidades não atendidas, oferecendo insights sobre os pontos fortes e fracos de cada ferramenta.
image: "https://opengraph-image.blockeden.xyz/api/og-cuckoo-network?title=Feedback%20de%20Usu%C3%A1rios%20do%20Reddit%20sobre%20Principais%20Ferramentas%20de%20Chat%20LLM"
---

# Feedback de Usuários do Reddit sobre Principais Ferramentas de Chat LLM

**Visão Geral:** Este relatório analisa discussões no Reddit sobre quatro ferramentas populares de chat IA – **ChatGPT da OpenAI**, **Claude da Anthropic**, **Gemini (Bard) do Google** e **LLMs de código aberto** (por exemplo, modelos baseados em LLaMA). Resume os pontos problemáticos comuns relatados pelos usuários para cada uma, os recursos que mais frequentemente solicitam, necessidades não atendidas ou segmentos de usuários que se sentem desassistidos, e diferenças na percepção entre desenvolvedores, usuários casuais e usuários empresariais. Exemplos específicos e citações de tópicos do Reddit são incluídos para ilustrar esses pontos.

![Feedback de Usuários do Reddit sobre Principais Ferramentas de Chat LLM](https://opengraph-image.blockeden.xyz/api/og-cuckoo-network?title=Feedback%20de%20Usu%C3%A1rios%20do%20Reddit%20sobre%20Principais%20Ferramentas%20de%20Chat%20LLM)

## ChatGPT (OpenAI)

### Pontos Problemáticos Comuns e Limitações

- **Memória de contexto limitada:** Uma das principais reclamações é a incapacidade do ChatGPT de lidar com conversas longas ou documentos grandes sem esquecer detalhes anteriores. Os usuários frequentemente atingem o limite de comprimento de contexto (alguns milhares de tokens) e precisam truncar ou resumir informações. Um usuário observou *“aumentar o tamanho da janela de contexto seria de longe a maior melhoria... Esse é o limite que mais me incomoda”*. Quando o contexto é excedido, o ChatGPT esquece instruções ou conteúdos iniciais, levando a quedas frustrantes na qualidade durante a sessão.

- **Limites de mensagens para GPT-4:** Usuários do ChatGPT Plus lamentam o limite de 25 mensagens/3 horas no uso do GPT-4 (um limite presente em 2023). Atingir esse limite os força a esperar, interrompendo o trabalho. Usuários intensivos consideram essa limitação um grande ponto problemático.

- **Filtros de conteúdo rigorosos (“nerfs”):** Muitos usuários do Reddit sentem que o ChatGPT se tornou excessivamente restritivo, frequentemente recusando solicitações que versões anteriores atendiam. Um post altamente votado reclamou que *“praticamente tudo que você pede hoje em dia retorna um ‘Desculpe, não posso ajudar’... Como isso passou de ser a ferramenta mais útil para o equivalente ao Google Assistant?”*. Os usuários citam exemplos como o ChatGPT recusando-se a reformular *seus próprios* textos (por exemplo, credenciais de login) devido a uso indevido hipotético. Assinantes pagantes argumentam que *“alguma noção vaga de que o usuário pode fazer 'coisas ruins'... não deveria ser motivo para não exibir resultados”*, já que eles querem a saída do modelo e a usarão de forma responsável.

- **Alucinações e erros:** Apesar de sua capacidade avançada, o ChatGPT pode produzir informações incorretas ou fabricadas com confiança. Alguns usuários observaram que isso piorou com o tempo, suspeitando que o modelo foi “simplificado”. Por exemplo, um usuário de finanças disse que o ChatGPT costumava calcular métricas como NPV ou IRR corretamente, mas após atualizações *“estou recebendo tantas respostas erradas... ainda produz respostas erradas [mesmo após correção]. Realmente acredito que ficou muito mais burro desde as mudanças.”*. Essas imprecisões imprevisíveis corroem a confiança para tarefas que exigem precisão factual.

- **Saídas de código incompletas:** Desenvolvedores frequentemente usam o ChatGPT para ajuda em codificação, mas relatam que às vezes ele omite partes da solução ou trunca códigos longos. Um usuário compartilhou que o ChatGPT agora *“omite código, produz código inútil, e simplesmente falha naquilo que preciso que ele faça... Muitas vezes omite tanto código que nem sei como integrar sua solução.”* Isso força os usuários a fazerem prompts de acompanhamento para extrair o restante ou a costurar manualmente as respostas – um processo tedioso.

- **Preocupações com desempenho e tempo de atividade:** Existe uma percepção de que o desempenho do ChatGPT para usuários individuais diminuiu à medida que o uso empresarial aumentou. *“Acho que estão alocando largura de banda e poder de processamento para empresas e retirando dos usuários, o que é insuportável considerando o custo de uma assinatura!”* opinou um assinante Plus frustrado. Quedas ou lentidões durante horários de pico foram notadas anedoticamente, o que pode interromper fluxos de trabalho.

### Recursos ou Melhorias Frequentemente Solicitados

- **Janela de contexto/memória mais longa:** De longe, a melhoria mais solicitada é um comprimento de contexto maior. Os usuários querem ter conversas muito mais longas ou alimentar documentos grandes sem reinicializações. Muitos sugerem expandir o contexto do ChatGPT para corresponder à capacidade de 32K tokens do GPT-4 (atualmente disponível via API) ou além. Como um usuário colocou, *“GPT é melhor com contexto, e quando não se lembra daquele contexto inicial, fico frustrado... Se os rumores forem verdadeiros sobre PDFs de contexto, isso resolveria basicamente todos os meus problemas.”* Há uma alta demanda por recursos para fazer upload de documentos ou vincular dados pessoais para que o ChatGPT possa lembrar e referenciá-los durante uma sessão.

- **Manipulação de arquivos e integração:** Os usuários frequentemente pedem maneiras mais fáceis de alimentar arquivos ou dados no ChatGPT. Em discussões, as pessoas mencionam querer *“copiar e colar meu Google Drive e fazer funcionar”* ou ter plugins que permitam ao ChatGPT buscar diretamente contexto de arquivos pessoais. Alguns tentaram soluções alternativas (como plugins de leitura de PDF ou vinculação de Google Docs), mas reclamaram de erros e limites. Um usuário descreveu seu plugin ideal como um que *“funciona como o Link Reader, mas para arquivos pessoais... escolhendo quais partes do meu drive usar em uma conversa... isso resolveria basicamente todos os problemas que tenho com o GPT-4 atualmente.”*. Em resumo, melhor suporte nativo para conhecimento externo (além dos dados de treinamento) é um pedido popular.

- **Redução de limitação para usuários pagos:** Como muitos usuários Plus atingem o limite de mensagens do GPT-4, eles pedem limites mais altos ou uma opção para pagar mais por acesso ilimitado. O limite de 25 mensagens é visto como arbitrário e prejudicial ao uso intensivo. As pessoas prefeririam um modelo baseado em uso ou um limite mais alto para que sessões longas de resolução de problemas não sejam interrompidas.

- **Modos de moderação “sem censura” ou personalizados:** Um segmento de usuários gostaria de poder alternar a rigidez dos filtros de conteúdo, especialmente ao usar o ChatGPT para si mesmos (não para conteúdo público). Eles sentem que um modo “de pesquisa” ou “sem censura” – com avisos, mas sem recusas rígidas – permitiria explorar mais livremente. Como um usuário observou, clientes pagantes veem isso como uma ferramenta e acreditam *“Pago por [isso].”* Eles querem a opção de obter respostas mesmo em consultas limítrofes. Embora a OpenAI tenha que equilibrar a segurança, esses usuários sugerem uma bandeira ou configuração para relaxar as políticas em chats privados.

- **Melhoria na precisão factual e atualizações:** Os usuários comumente pedem conhecimento mais atualizado e menos alucinações. O corte de conhecimento do ChatGPT (setembro de 2021 em versões anteriores) era uma limitação frequentemente levantada no Reddit. A OpenAI desde então introduziu navegação e plugins, que alguns usuários aproveitam, mas outros simplesmente pedem que o modelo base seja atualizado mais frequentemente com novos dados. Reduzir erros óbvios – especialmente em domínios como matemática e codificação – é um desejo contínuo. Alguns desenvolvedores fornecem feedback quando o ChatGPT erra na esperança de melhoria do modelo.

- **Melhores saídas de código e ferramentas:** Desenvolvedores têm pedidos de recursos como um interpretador de código melhorado que não omita conteúdo e integração com IDEs ou controle de versão. (O plugin Code Interpreter da OpenAI – agora parte da “Análise de Dados Avançada” – foi um passo nessa direção e recebeu elogios.) Ainda assim, os usuários frequentemente pedem controle mais fino na geração de código: por exemplo, uma opção para gerar código completo, não filtrado, mesmo que seja longo, ou mecanismos para corrigir facilmente o código se a IA cometer um erro. Basicamente, eles querem que o ChatGPT se comporte mais como um assistente de codificação confiável sem precisar de vários prompts para refinar a resposta.

- **Perfis de usuário persistentes ou memória:** Outra melhoria que alguns mencionam é permitir que o ChatGPT se lembre de coisas sobre o usuário entre sessões (com consentimento). Por exemplo, lembrar o estilo de escrita de alguém ou que ele é um engenheiro de software, sem precisar relembrar a cada novo chat. Isso poderia se conectar ao ajuste fino da API ou a um recurso de “perfil”. Os usuários agora copiam manualmente o contexto importante para novos chats, então uma memória embutida para preferências pessoais economizaria tempo.

### Necessidades ou Segmentos de Usuários Desassistidos

- **Pesquisadores e estudantes com documentos longos:** Pessoas que querem que o ChatGPT analise artigos de pesquisa extensos, livros ou grandes conjuntos de dados se sentem desassistidas. Os limites atuais os forçam a dividir o texto ou se contentar com resumos. Esse segmento se beneficiaria muito de janelas de contexto maiores ou recursos para lidar com documentos longos (como evidenciado por inúmeros posts sobre tentativas de contornar limites de tokens).

- **Usuários buscando narrativa criativa ou role-play além dos limites:** Embora o ChatGPT seja frequentemente usado para escrita criativa, alguns contadores de histórias se sentem limitados pelo modelo esquecer pontos iniciais da trama em uma história longa ou recusar conteúdo adulto/terror. Eles recorrem a modelos alternativos ou hacks para continuar suas narrativas. Esses usuários criativos seriam melhor atendidos por uma versão do ChatGPT com memória mais longa e um pouco mais de flexibilidade em violência fictícia ou temas maduros (dentro do razoável). Como um escritor de ficção observou, quando a IA perde o fio da história, *“tenho que lembrá-la do formato ou contexto exato... fico frustrado que estava ótimo dois prompts atrás, mas agora tenho que atualizar a IA.”*.

- **Usuários avançados e especialistas em domínios:** Profissionais em campos especializados (**finanças**, **engenharia**, **medicina**) às vezes acham que as respostas do ChatGPT carecem de profundidade ou precisão em seu domínio, especialmente se as perguntas envolverem desenvolvimentos recentes. Esses usuários desejam conhecimento especializado mais confiável. Alguns tentaram ajuste fino via API ou GPTs personalizados. Aqueles que não podem ajustar gostariam de versões do ChatGPT específicas para domínios ou plugins que integrem bancos de dados confiáveis. Em sua forma padrão, o ChatGPT pode desatender usuários que precisam de informações altamente precisas e específicas de campo (eles frequentemente têm que verificar seu trabalho).

- **Usuários que precisam de conteúdo sem censura ou de casos extremos:** Uma minoria de usuários (hackers testando cenários de segurança, escritores de ficção extrema, etc.) acha as restrições de conteúdo do ChatGPT muito limitantes para suas necessidades. Eles estão atualmente desassistidos pelo produto oficial (já que evita explicitamente certos conteúdos). Esses usuários frequentemente experimentam prompts de jailbreak ou usam modelos de código aberto para obter as respostas desejadas. Esta é uma lacuna deliberada para a OpenAI (para manter a segurança), mas significa que esses usuários buscam em outros lugares.

- **Indivíduos e empresas preocupados com a privacidade:** Alguns usuários (especialmente em ambientes corporativos) se sentem desconfortáveis em enviar dados sensíveis para o ChatGPT devido a preocupações com a privacidade. A OpenAI tem políticas para não usar dados da API para treinamento, mas a interface web do ChatGPT historicamente não oferecia tais garantias até que um recurso de exclusão fosse adicionado. Empresas que lidam com dados confidenciais (jurídicos, de saúde, etc.) frequentemente sentem que não podem utilizar totalmente o ChatGPT, deixando suas necessidades desassistidas a menos que construam soluções auto-hospedadas. Por exemplo, um usuário do Reddit mencionou que sua empresa mudou para um LLM local por razões de privacidade. Até que instâncias on-prem ou privadas do ChatGPT estejam disponíveis, esse segmento permanece cauteloso ou usa fornecedores especializados menores.

### Diferenças na Percepção por Tipo de Usuário

- **Desenvolvedores/Usuários Técnicos:** Desenvolvedores tendem a ser tanto alguns dos maiores defensores quanto críticos mais severos do ChatGPT. Eles adoram sua capacidade de explicar código, gerar boilerplate e ajudar na depuração. No entanto, sentem intensamente suas limitações em contexto mais longo e precisão de código. Como um desenvolvedor reclamou, o ChatGPT começou a *“produzir código inútil”* e omitir partes importantes, o que *“me irrita... Não quero ter que dizer 'não seja preguiçoso' – só quero o resultado completo”*. Desenvolvedores frequentemente notam até mesmo mudanças sutis na qualidade após atualizações do modelo e têm sido muito vocais no Reddit sobre “nerfs” percebidos ou declínios na capacidade de codificação. Eles também testam os limites (construindo prompts complexos, encadeando ferramentas), então desejam recursos como contexto expandido, menos limites de mensagens e melhor integração com ferramentas de codificação. Em resumo, desenvolvedores valorizam o ChatGPT por acelerar tarefas rotineiras, mas são rápidos em apontar erros de lógica ou código – eles o veem como um assistente júnior que ainda precisa de supervisão.

- **Usuários Casuais/Diários:** Usuários mais casuais – aqueles que pedem conhecimento geral, conselhos ou diversão – frequentemente se maravilham com as capacidades do ChatGPT, mas têm suas próprias queixas. Uma frustração comum de usuários casuais é quando o ChatGPT recusa uma solicitação que parece inofensiva para eles (provavelmente acionando uma regra de política). O autor original em um tópico exemplificou isso, ficando *“tão irritado quando escrevo um prompt que não deveria ter problema e ele recusa agora”*. Usuários casuais também podem encontrar o corte de conhecimento (descobrindo que o bot não consegue lidar com eventos muito atuais a menos que seja explicitamente atualizado) e às vezes notam quando o ChatGPT dá uma resposta obviamente errada. Ao contrário dos desenvolvedores, eles podem não sempre verificar a IA, o que pode levar a decepção se agirem com base em um erro. Por outro lado, muitos usuários casuais acham que as respostas mais rápidas do ChatGPT Plus e a saída melhorada do GPT-4 valem os $20/mês – a menos que o problema de “recusa” ou outros limites estraguem a experiência. Eles geralmente querem um assistente útil e versátil e podem se frustrar quando o ChatGPT responde com declarações de política ou precisa de um prompt complexo para obter uma resposta simples.

- **Usuários Empresariais/Profissionais:** Usuários empresariais frequentemente abordam o ChatGPT do ponto de vista da produtividade e confiabilidade. Eles apreciam a redação rápida de e-mails, resumos de documentos ou geração de ideias. No entanto, estão preocupados com **segurança de dados**, consistência e integração em fluxos de trabalho. No Reddit, profissionais discutiram querer o ChatGPT em ferramentas como Outlook, Google Docs ou como uma API em seus sistemas internos. Alguns notaram que à medida que a OpenAI se volta para atender clientes empresariais, o foco do produto parece mudar: há uma sensação de que a experiência do usuário gratuito ou individual degradou ligeiramente (por exemplo, mais lento ou “menos inteligente”) à medida que a empresa se expandiu para atender clientes maiores. Verdadeiro ou não, isso destaca uma percepção: usuários empresariais querem confiabilidade e serviço prioritário, e usuários individuais se preocupam que agora são de segunda classe. Além disso, profissionais precisam de saídas corretas – uma resposta chamativa, mas errada, pode ser pior do que nenhuma resposta. Assim, esse segmento é sensível à precisão. Para eles, recursos como contexto mais longo (para leitura de contratos, análise de bases de código) e tempo de atividade garantido são cruciais. Eles provavelmente pagarão mais por níveis de serviço premium, desde que seus requisitos de conformidade e privacidade sejam atendidos. Algumas empresas até exploram implantações on-premise ou usam a API da OpenAI com regras estritas de manuseio de dados para satisfazer suas políticas de TI.

---

## Claude (Anthropic)

### Pontos Problemáticos Comuns e Limitações

- **Limites de uso e restrições de acesso:** Claude recebeu elogios por oferecer um modelo poderoso (Claude 2) gratuitamente, mas os usuários rapidamente encontraram limites de uso (especialmente no nível gratuito). Após um certo número de prompts ou uma grande quantidade de texto, Claude pode parar e dizer algo como *“Desculpe, preciso concluir esta conversa por enquanto. Por favor, volte mais tarde.”* Essa limitação frustra usuários que tratam Claude como um parceiro de codificação ou escrita estendido. Mesmo usuários do Claude Pro (pagos) *“não têm garantia de tempo ilimitado”*, como um usuário observou; atingir a cota ainda produz a mensagem “volte mais tarde”. Além disso, por muito tempo Claude foi oficialmente restrito geograficamente (inicialmente disponível apenas nos EUA/Reino Unido). Usuários internacionais no Reddit tiveram que usar VPNs ou plataformas de terceiros para acessá-lo, o que foi um inconveniente. Isso fez muitos usuários fora dos EUA se sentirem excluídos até que o acesso fosse ampliado.

- **Tendência a se desviar com entradas muito grandes:** O recurso principal de Claude é sua *janela de contexto de 100k tokens*, permitindo prompts extremamente longos. No entanto, alguns usuários notaram que quando você coloca dezenas de milhares de tokens em Claude, suas respostas podem se tornar menos focadas. *“100k é super útil, mas se não seguir as instruções corretamente e se desviar, não é tão útil,”* observou um usuário. Isso sugere que com contextos enormes, Claude pode se desviar ou começar a divagar, exigindo prompts cuidadosos para mantê-lo na tarefa. É uma limitação inerente a empurrar o contexto ao extremo – o modelo retém muito, mas às vezes “esquece” quais detalhes são mais relevantes, levando a pequenas alucinações ou tangentes fora do tópico.

- **Formatação inconsistente ou obediência a instruções:** Em comparações lado a lado, alguns usuários acharam Claude menos previsível em como segue certas diretivas. Por exemplo, Claude é descrito como *“mais humano nas interações. Mas segue menos estritamente as mensagens do sistema.”*. Isso significa que se você der a ele um formato fixo para seguir ou uma persona muito estrita, Claude pode se desviar mais do que o ChatGPT faria. Desenvolvedores que dependem de saídas determinísticas (como formatos JSON ou estilos específicos) às vezes ficam frustrados se Claude introduz comentários extras ou não adere rigidamente ao modelo.

- **Restrições de conteúdo e recusas:** Embora não seja tão frequentemente criticado quanto o ChatGPT, os filtros de segurança de Claude aparecem. A Anthropic projetou Claude com ênfase pesada em IA constitucional (fazendo a IA seguir diretrizes éticas). Os usuários geralmente acham que Claude está disposto a discutir uma ampla gama de tópicos, mas há casos em que Claude recusa solicitações que o ChatGPT pode permitir. Por exemplo, um usuário do Reddit observou *“ChatGPT tem menos restrições morais... ele explicará quais máscaras de gás são melhores para quais condições, enquanto Claude recusará”*. Isso sugere que Claude pode ser mais rigoroso sobre certos conselhos “sensíveis” (talvez tratando como orientação potencialmente perigosa). Outro usuário tentou um cenário de role-play divertido (“finja que foi abduzido por alienígenas”) que Claude recusou, enquanto Gemini e ChatGPT se envolveriam. Portanto, Claude tem filtros que podem ocasionalmente surpreender usuários que esperam que ele seja mais permissivo.

- **Falta de capacidades multimodais:** Ao contrário do ChatGPT (que, no final de 2023, ganhou compreensão de imagens com o GPT-4 Vision), Claude é atualmente apenas texto. Usuários do Reddit notam que Claude não pode analisar imagens ou navegar na web por conta própria. Isso não é exatamente um “ponto problemático” (a Anthropic nunca anunciou esses recursos), mas é uma limitação em relação aos concorrentes. Usuários que querem uma IA para interpretar um diagrama ou captura de tela não podem usar Claude para isso, enquanto o ChatGPT ou Gemini podem lidar com isso. Da mesma forma, qualquer recuperação de informações atuais requer o uso de Claude via uma ferramenta de terceiros (por exemplo, integração com Poe ou mecanismo de busca), já que Claude não tem um modo de navegação oficial neste momento.

- **Problemas menores de estabilidade:** Alguns usuários relataram que Claude ocasionalmente é repetitivo ou fica preso em loops para certos prompts (embora isso seja menos comum do que com alguns modelos menores). Além disso, versões anteriores de Claude às vezes terminavam respostas prematuramente ou demoravam muito com saídas longas, o que pode ser visto como pequenos aborrecimentos, embora Claude 2 tenha melhorado na velocidade.

### Recursos ou Melhorias Frequentemente Solicitados

- **Limites de uso mais altos ou ajustáveis:** Entusiastas de Claude no Reddit frequentemente pedem à Anthropic para aumentar os limites de conversa. Eles gostariam de usar o contexto de 100k ao máximo sem atingir uma parada artificial. Alguns sugerem que mesmo o Claude Pro pago deveria permitir *significativamente* mais tokens por dia. Outros sugeriram a ideia de um “modo estendido de 100k” opcional – por exemplo, *“Claude deveria ter um modo de contexto de 100k com o dobro dos limites de uso”* – onde talvez uma assinatura pudesse oferecer acesso expandido para usuários intensivos. Em essência, há demanda por um plano que concorra com o uso ilimitado (ou de alto limite) do ChatGPT para assinantes.

- **Melhor navegação de contexto longo:** Embora ter 100k tokens seja inovador, os usuários querem que Claude utilize melhor esse contexto. Uma melhoria seria refinar como Claude prioriza informações para que permaneça na tarefa. A Anthropic poderia trabalhar na adesão do modelo ao prompt quando o prompt é enorme. Discussões no Reddit sugerem técnicas como permitir que o usuário “fixe” certas instruções para que não sejam diluídas em um grande contexto. Quaisquer ferramentas para ajudar a segmentar ou resumir partes da entrada também poderiam ajudar Claude a lidar com entradas grandes de forma mais coerente. Em suma, os usuários adoram a possibilidade de alimentar um livro inteiro para Claude – eles só querem que ele permaneça afiado durante todo o processo.

- **Plugins ou navegação na web:** Muitos usuários do ChatGPT se acostumaram com plugins (por exemplo, navegação, execução de código, etc.) e expressam interesse em Claude ter extensibilidade semelhante. Um pedido comum é que Claude tenha uma função oficial de busca/navegação na web, para que possa buscar informações atualizadas sob demanda. Atualmente, o conhecimento de Claude é principalmente estático (dados de treinamento até o início de 2023, com algumas atualizações). Se Claude pudesse consultar a web, isso aliviaria essa limitação. Da mesma forma, um sistema de plugins onde Claude pudesse usar ferramentas de terceiros (como calculadoras ou conectores de banco de dados) poderia expandir sua utilidade para usuários avançados. Isso continua sendo um recurso que Claude não possui, e usuários do Reddit frequentemente mencionam como o ecossistema de plugins do ChatGPT lhe dá uma vantagem em certas tarefas.

- **Entrada multimodal (imagens ou áudio):** Alguns usuários também se perguntaram se Claude suportará entradas de imagem ou gerará imagens. O Gemini do Google e o GPT-4 da OpenAI têm capacidades multimodais, então para se manter competitivo, os usuários esperam que a Anthropic explore isso. Um pedido frequente é: *“Posso fazer upload de um PDF ou uma imagem para Claude analisar?”* Atualmente, a resposta é não (além de soluções alternativas como converter imagens em texto em outro lugar). Mesmo apenas permitir imagem-para-texto (OCR e descrição) satisfaria muitos que querem um assistente tudo-em-um. Isso está na lista de desejos, embora a Anthropic não tenha anunciado nada semelhante até o início de 2025.

- **Ajuste fino ou personalização:** Usuários avançados e empresas às vezes perguntam se podem ajustar Claude em seus próprios dados ou obter versões personalizadas. A OpenAI oferece ajuste fino para alguns modelos (ainda não para o GPT-4, mas para o GPT-3.5). A Anthropic lançou uma interface de ajuste fino para o Claude 1.3 anteriormente, mas não é amplamente divulgada para o Claude 2. Usuários do Reddit perguntaram sobre a possibilidade de treinar Claude no conhecimento da empresa ou estilo de escrita pessoal. Uma maneira mais fácil de fazer isso (além de injeções de prompt a cada vez) seria muito bem-vinda, pois poderia transformar Claude em um assistente personalizado que lembra uma base de conhecimento ou persona específica.

- **Disponibilidade mais ampla:** Usuários fora dos EUA frequentemente pedem que Claude seja oficialmente lançado em seus países. Posts do Canadá, Europa, Índia, etc., perguntam quando poderão usar o site do Claude sem uma VPN ou quando a API do Claude estará aberta mais amplamente. A Anthropic tem sido cautelosa, mas a demanda é global – provavelmente uma melhoria aos olhos de muitos seria simplesmente “deixar mais de nós usá-lo.” A expansão gradual do acesso pela empresa abordou parcialmente isso.

### Necessidades ou Segmentos de Usuários Desassistidos

- **Base de usuários internacional:** Como mencionado, por muito tempo a base de usuários primária de Claude foi limitada pela geografia. Isso deixou muitos *potenciais* usuários desassistidos. Por exemplo, um desenvolvedor na Alemanha interessado no contexto de 100k de Claude não tinha uma maneira oficial de usá-lo. Embora existam soluções alternativas (plataformas de terceiros, ou VPN + verificação de telefone em um país suportado), essas barreiras significavam que usuários internacionais casuais estavam efetivamente bloqueados. Em contraste, o ChatGPT está disponível na maioria dos países. Portanto, falantes de inglês não americanos e especialmente falantes de outras línguas foram desassistidos pelo lançamento limitado de Claude. Eles podem ainda depender do ChatGPT ou de modelos locais simplesmente devido a problemas de acesso.

- **Usuários que precisam de formatação de saída estrita:** Como mencionado, Claude às vezes toma liberdades nas respostas. Usuários que precisam de saídas altamente estruturadas (como JSON para uma aplicação, ou uma resposta seguindo um formato preciso) podem achar Claude menos confiável para isso do que o ChatGPT. Esses usuários – frequentemente desenvolvedores integrando a IA em um sistema – são um segmento que poderia ser melhor atendido se Claude permitisse um “modo estrito” ou melhorasse sua adesão a instruções. Eles atualmente podem evitar Claude para tais tarefas, ficando com modelos conhecidos por seguir formatos mais rigidamente.

- **Usuários casuais de perguntas e respostas (vs. usuários criativos):** Claude é frequentemente elogiado por tarefas criativas – ele produz prosa fluente, semelhante à humana, e ensaios reflexivos. No entanto, alguns usuários no Reddit notaram que para perguntas-respostas diretas ou consultas factuais, Claude às vezes dá respostas verbosas onde a brevidade seria suficiente. O usuário que comparou ChatGPT e Claude disse que o ChatGPT tende a ser sucinto e em tópicos, enquanto Claude dá mais narrativa por padrão. Usuários que só querem uma resposta factual rápida (como “Qual é a capital de X e sua população?”) podem sentir que Claude é um pouco indireto. Esses usuários são melhor atendidos por algo como uma busca precisa ou um modelo conciso. Claude pode fazer isso se solicitado, mas seu estilo pode não corresponder à expectativa de uma pergunta-resposta concisa, significando que esse segmento pode recorrer a outras ferramentas (como Bing Chat ou Google).

- **Usuários críticos de segurança:** Por outro lado, alguns usuários que *exigem* aderência muito cuidadosa à segurança (por exemplo, educadores usando IA com alunos, ou clientes empresariais que querem zero risco de saídas descontroladas) podem considerar o alinhamento de Claude um ponto positivo, mas como o ChatGPT também é bastante alinhado e tem mais recursos empresariais, esses usuários podem não escolher especificamente Claude. É um pequeno segmento, mas pode-se argumentar que Claude ainda não o capturou distintamente. Eles podem estar desassistidos no sentido de que não têm uma maneira fácil de *aumentar* as salvaguardas de Claude ou ver sua “cadeia de pensamento” (que a Anthropic tem internamente via a abordagem de IA constitucional, mas os usuários finais não interagem diretamente com isso além de notar o tom geralmente educado de Claude).

- **Falantes de outras línguas (qualidade da saída):** Claude foi treinado principalmente em inglês (como a maioria dos grandes LLMs). Alguns usuários o testaram em outras línguas; ele pode responder em muitas, mas a qualidade pode variar. Se, por exemplo, um usuário quer uma resposta muito nuançada em francês ou hindi, é possível que as habilidades de Claude não sejam tão refinadas lá quanto as do ChatGPT (o GPT-4 demonstrou forte desempenho multilíngue, muitas vezes superior a outros modelos em certos benchmarks). Usuários que conversam principalmente em línguas diferentes do inglês podem achar a fluência ou precisão de Claude ligeiramente mais fraca. Este segmento é um pouco desassistido simplesmente porque a Anthropic não destacou o treinamento multilíngue como uma prioridade publicamente.

### Diferenças na Percepção por Tipo de Usuário

- **Desenvolvedores/Usuários Técnicos:** Desenvolvedores no Reddit têm elogiado cada vez mais Claude, especialmente Claude 2 / Claude 3.5, para tarefas de codificação. A mudança de percepção no final de 2024 foi notável: muitos desenvolvedores começaram a preferir Claude ao ChatGPT para assistência em programação. Eles citam desempenho *“incrível em codificação”* e a capacidade de lidar com bases de código maiores de uma só vez. Por exemplo, um usuário escreveu *“Claude Sonnet 3.5 é melhor para trabalhar com código (analisar, gerar) [do que o ChatGPT].”* Desenvolvedores apreciam que Claude pode pegar um grande pedaço de código de projeto ou logs e produzir análises ou melhorias coerentes, graças ao seu enorme contexto. No entanto, eles também notam suas peculiaridades – como às vezes injetar mais floreios conversacionais ou não seguir uma especificação à risca. No geral, muitos desenvolvedores mantêm tanto o ChatGPT quanto o Claude à mão: um para lógica rigorosa passo a passo (ChatGPT) e outro para contexto amplo e compreensão empática (Claude). É revelador que um comentarista disse *“Se eu tivesse que escolher um, escolheria Claude”* após compará-los diariamente. Isso indica uma percepção muito positiva entre usuários avançados, especialmente para casos de uso como brainstorming, revisão de código ou sugestões arquitetônicas. A única queixa comum dos desenvolvedores é atingir os limites de uso de Claude quando tentam forçá-lo (por exemplo, alimentando um prompt de 50K tokens para analisar um repositório inteiro). Em resumo, os desenvolvedores veem Claude como uma ferramenta extremamente poderosa – em alguns casos superior ao ChatGPT – limitada apenas pela disponibilidade e alguma imprevisibilidade na formatação.

- **Usuários Casuais/Não Técnicos:** Usuários casuais que experimentaram Claude frequentemente comentam sobre como ele é *amigável e articulado*. O estilo de Claude tende a ser conversacional, educado e detalhado. Um novo usuário comparando-o ao ChatGPT observou que *“Claude é mais empático e segue um tom de conversa... ChatGPT tende a usar tópicos com muita frequência”*. Essa calorosa semelhança humana torna Claude atraente para pessoas que o usam para escrita criativa, conselhos ou apenas para conversar por informações. Alguns até personificam Claude como tendo uma “personalidade” que é compassiva. Usuários casuais também gostam que a versão gratuita de Claude permitia acesso a um nível de inteligência equivalente ao GPT-4 sem uma assinatura (pelo menos até os limites de taxa). Por outro lado, usuários casuais esbarram nas recusas de Claude em certos tópicos e podem não entender por quê (já que Claude o fraseia de forma apologética, mas firme). Se um usuário casual pediu algo limítrofe e recebeu uma recusa de Claude, pode perceber como menos capaz ou muito restrito, não percebendo que é uma postura de política. Outro aspecto é que Claude carece de reconhecimento de nome – muitos usuários casuais podem nem saber que devem experimentá-lo, a menos que estejam conectados a comunidades de IA. Aqueles que experimentam geralmente comentam que parece *“como conversar com um humano”* de uma maneira boa. Eles tendem a estar muito satisfeitos com a capacidade de Claude de lidar com perguntas abertas ou pessoais. Portanto, a percepção do usuário casual é amplamente positiva em relação à *qualidade e tom da saída de Claude*, com alguma confusão ou frustração em torno de sua disponibilidade (tendo que usá-lo em um aplicativo ou região específica) e momentos ocasionais de “não posso fazer isso”.

- **Usuários Empresariais/Profissionais:** As percepções empresariais de Claude são um pouco mais difíceis de avaliar a partir do Reddit público (já que menos usuários empresariais postam em detalhes), mas algumas tendências emergem. Primeiro, a Anthropic posicionou Claude como mais *focado em privacidade* e disposto a assinar acordos empresariais – isso atrai empresas preocupadas com dados com a OpenAI. De fato, algumas discussões no Reddit mencionam Claude no contexto de ferramentas como Slack ou Notion, onde está integrado como assistente. Profissionais que usaram essas integrações podem nem perceber que Claude é o motor, mas quando percebem, o comparam favoravelmente em termos de estilo de escrita e capacidade de digerir grandes documentos corporativos. Por exemplo, uma equipe pode alimentar um longo relatório trimestral para Claude e obter um resumo decente – algo que o contexto menor do ChatGPT teria dificuldade em fazer. Dito isso, usuários empresariais também notam a falta de certos recursos do ecossistema; por exemplo, a OpenAI oferece controle de mensagens do sistema, chamadas de função, etc., em sua API, que a Anthropic tem suporte mais limitado. Um desenvolvedor trabalhando em uma solução empresarial comentou que *Claude é mais direcionável em conversas, enquanto o ChatGPT tende a ser mais rígido... [mas] o ChatGPT tem acesso à web, o que pode ser muito útil*. A implicação é que para tarefas de pesquisa ou consulta de dados que um usuário empresarial pode precisar (como inteligência competitiva), o ChatGPT pode buscar informações diretamente, enquanto Claude exigiria uma etapa separada. No geral, os usuários empresariais parecem ver Claude como uma IA muito competente – em alguns casos *melhor* para tarefas analíticas internas – mas talvez não tão rica em recursos ainda para integração. O custo é outro fator: o preço e os termos da API de Claude não são tão públicos quanto os da OpenAI, e algumas startups no Reddit mencionaram incerteza sobre o preço ou estabilidade de Claude. Em resumo, os profissionais respeitam as capacidades de Claude (especialmente sua confiabilidade em seguir instruções de alto nível e resumir entradas grandes), mas eles ficam de olho em como ele evolui em termos de integração, suporte e disponibilidade global antes de se comprometerem totalmente com ele em detrimento do ChatGPT mais estabelecido.

---

## Google Gemini (Bard)

### Pontos Problemáticos Comuns e Limitações

- **Respostas imprecisas ou “burras”:** Um fluxo de feedback do Reddit apareceu quando o Google lançou sua atualização Bard com tecnologia Gemini, muito dele negativo. Usuários reclamaram que o Gemini **desempenhou mal em QA básico** em comparação com o ChatGPT. Uma avaliação direta intitulada “Opinião 100% Honesta sobre o Google Gemini” afirmou: *“É um chatbot LLM quebrado e impreciso”*. Outro usuário frustrado perguntou: *“Como o Gemini ainda é tão ruim? O número de vezes que peço algo ao Gemini e ele me dá respostas incorretas ou incompletas é ridículo”*. Eles o compararam lado a lado com o ChatGPT-4 e descobriram que o ChatGPT deu *“resposta perfeita, correta e eficiente de uma só vez,”* enquanto o Gemini divagou e exigiu vários prompts para chegar a uma resposta meio satisfatória. Em essência, os primeiros usuários sentiram que o Gemini frequentemente **alucinava ou perdia o ponto** das perguntas, exigindo esforço excessivo de prompt para extrair informações corretas. Essa inconsistência na qualidade foi uma grande decepção, dada a expectativa em torno do Gemini.

- **Verborragia e floreios excessivos:** Muitos usuários notaram que o Gemini (na forma do novo Bard) tende a produzir respostas prolixas que não vão direto ao ponto. Como uma pessoa descreveu, *“Ele divagou... 3 parágrafos de lixo de IA... mesmo assim, [apenas] eventualmente mencionou a resposta enterrada em parágrafos de lixo”*. Isso contrasta fortemente com o ChatGPT, que frequentemente oferece respostas mais concisas ou tópicos quando apropriado. A verborragia se torna um ponto problemático quando os usuários têm que peneirar muito texto para um simples fato. Alguns especularam que o Google pode ter ajustado para ser conversacional ou “útil”, mas exagerou em *muita* explicação sem substância.

- **Integração ruim com os próprios serviços do Google:** Um dos pontos de venda do assistente de IA do Google deveria ser a integração com o ecossistema do Google (Gmail, Docs, Drive, etc.). No entanto, as primeiras experiências dos usuários foram muito decepcionantes nesse aspecto. Um usuário desabafou: *“Nem me faça começar sobre sua quase total incapacidade de se integrar aos próprios produtos do Google, que deveria ser um ‘recurso’ (que aparentemente ele não sabe que tem).”*. Por exemplo, as pessoas tentariam pedir ao Gemini (via Bard) para resumir um Google Doc ou redigir um e-mail com base em algumas informações – recursos que o Google anunciou – e o bot responderia que **não pode acessar esses dados**. Um usuário no r/GooglePixel escreveu: *“Toda vez que tento usar o Gemini com meus Google Docs ou Drive, ele me diz que não pode fazer nada com isso. Qual é o ponto de ter esses recursos de integração?”*. Isso mostra uma lacuna significativa entre as capacidades prometidas e o desempenho real, deixando os usuários sentindo que o “assistente de IA” não está ajudando muito dentro do próprio ecossistema do Google.

- **Recusas e confusão de capacidade:** Os usuários também encontraram recusas bizarras ou contradições do Gemini. O mesmo usuário do Reddit observou que o Gemini *“recusa fazer coisas sem motivo, esquece que pode fazer outras coisas... Outro dia me disse que não tinha acesso à internet/dados ao vivo. O quê.”*. Isso indica que o Gemini às vezes **declina tarefas que deveria ser capaz de fazer** (como recuperar informações ao vivo, às quais o Bard está conectado) ou faz declarações incorretas sobre suas próprias habilidades. Tais experiências deram a impressão de uma IA que não é apenas menos inteligente, mas também **menos confiável ou autoconsciente**. Outro comentário colorido de um usuário: *“Gemini é um lixo absoluto. Você já teve um daqueles momentos em que só quer jogar as mãos para cima e dizer: 'O que eles estavam pensando?'”* encapsula a frustração. Essencialmente, os problemas de integração e consistência do produto do Gemini fizeram com que ele parecesse *inacabado* para muitos dos primeiros adotantes.

- **Habilidades de codificação pouco notáveis:** Embora não seja tão amplamente discutido quanto o Q&A geral, vários usuários testaram o Gemini (Bard) em tarefas de codificação e o acharam inferior. Em fóruns de IA, as capacidades de codificação do Gemini geralmente foram classificadas abaixo do GPT-4 e até mesmo abaixo do Claude. Por exemplo, um usuário afirmou claramente que *“Claude 3.5 Sonnet é claramente melhor para codificação do que o ChatGPT 4o... Gemini é um lixo absoluto [nesse contexto]”*. O consenso era que o Gemini poderia escrever código simples ou explicar algoritmos básicos, mas frequentemente tropeçava em problemas mais complexos ou produzia código com erros. Sua falta de um conjunto amplo de ferramentas para desenvolvedores (por exemplo, não tem um equivalente do Code Interpreter ou chamadas de função robustas) também significava que não era a primeira escolha para programadores. Então, embora nem todo usuário casual se importe com código, isso é uma limitação para esse segmento.

- **Limitações em dispositivos móveis:** O Gemini foi lançado como parte do Assistente do Google em telefones Pixel (marcado como “Assistente com Bard”). Alguns usuários do Pixel notaram que usá-lo como substituto do assistente de voz tinha problemas. Às vezes, não captava prompts de voz com precisão ou demorava muito para responder em comparação com o antigo Assistente do Google. Também houve comentários sobre a necessidade de optar por isso e perder alguns recursos clássicos do Assistente. Isso criou a percepção de que *a integração do Gemini em dispositivos não estava totalmente pronta*, deixando usuários avançados do ecossistema do Google sentindo que tinham que escolher entre um assistente inteligente e um funcional.

### Recursos ou Melhorias Frequentemente Solicitados

- **Melhoria dramática na precisão e raciocínio:** A melhoria número um que os usuários querem para o Gemini é simplesmente **ser mais inteligente e confiável**. O feedback do Reddit deixa claro que o Google precisa fechar a lacuna na qualidade das respostas. Os usuários esperam que o Gemini utilize o vasto acesso a informações do Google para dar *respostas factuais e diretas*, não respostas prolixas ou incorretas. Então, os pedidos (frequentemente formulados sarcasticamente) se resumem a: *faça-o tão bom quanto ou melhor que o GPT-4 em conhecimento geral e raciocínio.* Isso inclui melhor manuseio de perguntas de acompanhamento e prompts complexos. Essencialmente, “conserte o cérebro” do Gemini – aproveite essas supostas vantagens de treinamento multimodal para que ele pare de perder detalhes óbvios. O Google provavelmente ouviu isso alto e claro: muitos posts comparam respostas específicas onde o ChatGPT se destacou e o Gemini falhou, o que serve como relatórios de bugs informais para melhoria.

- **Melhor integração e consciência de contexto:** Os usuários querem que o Gemini cumpra a promessa de um assistente de ecossistema Google perfeito. Isso significa que ele deve **interagir adequadamente com Gmail, Calendário, Docs, Drive, etc.** Se um usuário pedir “Resuma o documento que abri” ou “Redija uma resposta para o último e-mail do meu chefe”, a IA deve fazê-lo – e fazê-lo com segurança. Agora, o pedido é que o Google *habilite esses recursos e faça o Gemini realmente reconhecer quando tal tarefa é possível*. Foi anunciado que o Bard poderia se conectar ao conteúdo do usuário (com permissão), então os usuários estão efetivamente exigindo que o Google “ligue” ou conserte essa integração. Este é um recurso chave especialmente para usuários empresariais. Além disso, na frente de navegação na web: o Bard (Gemini) pode pesquisar na web, mas alguns usuários querem que ele cite fontes mais claramente ou seja mais oportuno na incorporação de notícias de última hora. Então, melhorar a natureza *conectada* do Gemini é um pedido frequente.

- **Controles de concisão:** Dadas as reclamações de verborragia, alguns usuários sugerem um recurso para alternar o estilo de resposta. Por exemplo, um *“modo breve”* onde o Gemini dá uma resposta curta e direta por padrão, a menos que solicitado a elaborar. Por outro lado, talvez um “modo detalhado” para aqueles que querem respostas muito completas. O ChatGPT implicitamente permite parte disso pelo prompt do usuário (“mantenha breve”); com o Gemini, os usuários sentiram que mesmo quando não pediam detalhes, ele explicava demais. Então, uma configuração embutida ou apenas um melhor ajuste para produzir respostas concisas quando apropriado seria uma melhoria bem-vinda. Em essência, ajuste o dial de verborragia.

- **Paridade de recursos com o ChatGPT (codificação, plugins, etc.):** Usuários avançados no Reddit comparam explicitamente recursos. Eles pedem que o Gemini/Bard do Google ofereça coisas como um *sandbox de execução de código* (semelhante ao Code Interpreter do ChatGPT), a capacidade de fazer upload de imagens/PDFs para análise (já que o Gemini é multimodal, os usuários querem realmente alimentá-lo com imagens personalizadas, não apenas fazer com que ele descreva as fornecidas). Outro recurso frequentemente mencionado é melhor **memória dentro da conversa** – enquanto o Bard tem alguma memória de interações passadas, os usuários querem que ele seja tão bom quanto o ChatGPT em referenciar contexto anterior, ou até mesmo ter armazenamento de conversa persistente como o histórico de chat do ChatGPT que você pode rolar e revisitar. Essencialmente, o Google está sendo solicitado a alcançar todos os recursos de qualidade de vida que os usuários do ChatGPT Plus têm: histórico de chat, ecossistema de plugins (ou pelo menos fortes integrações de terceiros), assistência de codificação, etc.

- **Melhorias em aplicativos móveis e voz:** Muitos usuários casuais solicitaram um **aplicativo móvel dedicado para Bard/Gemini** (semelhante ao aplicativo móvel do ChatGPT). Confiar em uma interface web ou apenas no Assistente Pixel é limitante. Um aplicativo oficial em iOS/Android com entrada de voz, respostas faladas (para uma sensação de assistente verdadeiro) e integração estreita poderia melhorar muito a experiência do usuário. Junto com isso, os proprietários de Pixel querem que o Assistente com Bard fique mais rápido e mais funcional – basicamente, eles querem o melhor do antigo Assistente do Google (ações rápidas e precisas) combinado com a inteligência do Gemini. Por exemplo, coisas como continuar permitindo comandos de voz “Hey Google” para dispositivos inteligentes e não apenas respostas conversacionais. O Google poderia melhorar o modo de voz do Gemini para realmente substituir o assistente legado sem regressões de recursos.

- **Transparência e controle:** Alguns usuários pediram mais insights sobre as fontes do Bard ou uma maneira de ajustar seu estilo. Por exemplo, mostrar de qual resultado do Google o Bard está extraindo informações (para verificar a precisão) – algo que o Bing Chat faz citando links. Além disso, porque o Bard ocasionalmente produz informações erradas, os usuários querem poder sinalizar ou corrigir isso, e idealmente o Bard deveria aprender com esse feedback ao longo do tempo. Ter um mecanismo de feedback fácil (“polegar para baixo – isso está incorreto porque...”) que leva a uma rápida melhoria do modelo instilaria confiança de que o Google está ouvindo. Basicamente, recursos para tornar a IA mais um assistente colaborativo do que uma caixa preta.

### Necessidades ou Segmentos de Usuários Desassistidos

- **Usuários buscando um assistente pessoal confiável:** Ironicamente, o grupo que o Google *almejou* – pessoas que querem um assistente pessoal poderoso – se sente mais desassistido pelo Gemini em sua forma atual. Os primeiros adotantes que ativaram o novo Assistente baseado no Bard esperavam uma atualização, mas muitos sentiram que foi um downgrade em termos práticos. Por exemplo, se alguém quer um assistente de voz para *responder com precisão* a curiosidades, definir lembretes, controlar dispositivos e integrar informações de suas contas, o Gemini teve dificuldades. Isso deixou o próprio segmento de profissionais ocupados ou entusiastas de gadgets (que dependem de assistentes para produtividade) sentindo que suas necessidades não foram atendidas. Um usuário comentou que consideraria pagar pelo “Assistente com Bard” do Pixel *“se [ele] superar o Assistente do Google”*, implicando que ainda não havia. Então, esse segmento ainda está esperando por um assistente de IA confiável e genuinamente útil – eles o adotarão se o Gemini melhorar.

- **Falantes não nativos de inglês / localização:** Os produtos do Google geralmente têm excelente localização, mas não está claro se o Bard/Gemini foi igualmente forte em todas as línguas no lançamento. Alguns usuários internacionais relataram que as respostas do Bard em sua língua nativa eram menos fluentes ou úteis, empurrando-os de volta para concorrentes locais. Se os dados de treinamento ou otimização do Gemini favoreceram o inglês, então usuários não falantes de inglês são desassistidos. Eles podem preferir o ChatGPT ou modelos locais que tenham capacidades multilíngues explicitamente otimizadas. Este é um espaço em que o Google poderia tradicionalmente se destacar (dado sua tecnologia de tradução), mas o feedback dos usuários sobre isso é escasso – provavelmente indicando que o Gemini ainda não impressionou essas comunidades.

- **Clientes empresariais (até agora):** Grandes organizações não adotaram amplamente o Bard/Gemini com base em conversas públicas, muitas vezes devido a lacunas de confiança e capacidade. As empresas precisam de consistência, citações e integração com seus fluxos de trabalho (o Office 365 está profundamente integrado com a tecnologia da OpenAI via MS Copilot, por exemplo). O equivalente do Google (Duet AI com Gemini) ainda está evoluindo. Até que o Gemini/Bard prove que pode redigir e-mails de forma confiável, criar apresentações ou analisar dados no Google Sheets em um nível igual ou superior ao GPT-4, os usuários empresariais sentirão que a solução do Google não está atendendo completamente suas necessidades. Alguns posts no r/Bard de profissionais são no sentido de “Tentei o Bard para tarefas de trabalho, não foi tão bom quanto o ChatGPT, então vamos esperar e ver.” Isso indica que os usuários empresariais são um segmento desassistido por enquanto – eles querem uma IA que se encaixe no Google Workspace e realmente aumente a produtividade sem precisar de verificação constante das saídas.

- **Usuários no ecossistema do Google que preferem soluções únicas:** Há um segmento de usuários que usa o Google para tudo (pesquisa, e-mail, documentos) e *gostaria* de usar uma IA do Google para todas as suas necessidades de chatbot – se fosse tão boa. No momento, esses usuários estão um pouco desassistidos porque acabam usando o ChatGPT para certas coisas e o Bard para outras. Eles podem fazer perguntas factuais ao ChatGPT porque confiam mais na qualidade das respostas, mas usam o Bard para suas tentativas de navegação ou integração. Essa experiência dividida não é ideal. Esses usuários realmente só querem ficar em um aplicativo/assistente. Se o Gemini melhorar, eles se consolidarão em torno dele, mas até lá seu caso de uso de “um assistente para governar todos” não está sendo cumprido.

- **Desenvolvedores/Cientistas de dados no Google Cloud:** O Google lançou modelos Gemini via sua plataforma Vertex AI para desenvolvedores. No entanto, os primeiros relatórios e benchmarks sugeriram que o Gemini (particularmente o modelo “Gemini Pro” disponível) não estava superando o GPT-4. Desenvolvedores que preferem o Google Cloud para serviços de IA são assim um pouco desassistidos pela qualidade do modelo – eles têm que aceitar um modelo ligeiramente inferior ou integrar a API da OpenAI separadamente. Este segmento de desenvolvedores empresariais está ansioso por um modelo forte do Google para que possam manter tudo em uma pilha. Até que o desempenho do Gemini se destaque claramente em algumas áreas ou o preço ofereça uma razão convincente, ele não está atendendo totalmente às necessidades desse grupo em termos competitivos.

### Diferenças na Percepção por Tipo de Usuário

- **Desenvolvedores/Entusiastas de Tecnologia:** Usuários experientes em tecnologia abordaram o Gemini com altas expectativas (afinal, é o Google). Sua percepção rapidamente azedou após testes práticos. Muitos desenvolvedores no Reddit realizaram benchmarks ou suas perguntas difíceis favoritas através do Gemini e o acharam deficiente. Um programador afirmou sem rodeios, *“Gemini é um lixo absoluto como o Llama 3.0 costumava ser”*, indicando que o classificam até mesmo abaixo de alguns modelos abertos. Desenvolvedores são particularmente sensíveis a erros lógicos e verborragia. Então, quando o Gemini deu respostas verbosas, mas incorretas, perdeu credibilidade rapidamente. Por outro lado, os desenvolvedores reconhecem o potencial do Google; alguns têm esperança de que *“com mais ajuste fino, o Gemini melhorará”* e eles o testam periodicamente após atualizações. No momento, no entanto, a maioria dos desenvolvedores o percebe como **inferior ao GPT-4** em quase todas as tarefas sérias (codificação, resolução de problemas complexos). Eles apreciam certas coisas: por exemplo, o Gemini tem acesso a informações em tempo real (via pesquisa do Google) sem precisar de um plugin, o que é útil para consultas atualizadas. Um desenvolvedor pode usar o Bard para algo como “pesquisar e resumir os últimos artigos sobre X,” onde ele pode citar dados da web. Mas para raciocínio autônomo, eles tendem a outros modelos. Em resumo, entusiastas de tecnologia veem o Gemini como um trabalho promissor em andamento que *atualmente* parece uma geração atrás. Ele não ganhou sua confiança total, e eles frequentemente postam comparações lado a lado destacando seus erros para estimular o Google a melhorá-lo.

- **Usuários Casuais/Diários:** Usuários casuais, incluindo aqueles que tiveram acesso ao novo Bard em seus telefones ou via web, tiveram sentimentos mistos. Muitos usuários casuais inicialmente abordaram o Bard (Gemini) porque é gratuito e fácil de acessar com uma conta do Google, ao contrário do GPT-4, que era pago. Alguns usuários casuais realmente relatam experiências decentes para usos simples: por exemplo, um Redditor no r/Bard deu uma avaliação positiva, observando que o Gemini os ajudou com coisas como revisar documentos legais, redação publicitária e até mesmo um caso de uso divertido de identificar tamanhos de roupas a partir de uma foto. Eles disseram *“O Gemini tem sido um recurso valioso para responder às minhas perguntas... informações atualizadas... Eu me acostumei tanto com a versão paga que não consigo me lembrar de como a versão gratuita funciona.”* – indicando que pelo menos *alguns* usuários casuais que investiram tempo (e dinheiro) no Bard Advanced o acharam útil no dia a dia. Esses usuários tendem a usá-lo para ajuda prática e cotidiana e podem não forçar o modelo ao limite. No entanto, muitos outros usuários casuais (especialmente aqueles que também experimentaram o ChatGPT) ficaram desapontados. Pessoas comuns pedindo coisas como conselhos de viagem, curiosidades ou ajuda com uma tarefa acharam as respostas do Bard menos claras ou úteis. A percepção aqui é dividida: **usuários leais à marca Google** vs. **aqueles já acostumados com o ChatGPT**. O primeiro grupo, se não usou muito o ChatGPT, às vezes acha o Bard/Gemini “muito bom” para suas necessidades e aprecia que ele está integrado à pesquisa e é gratuito. O segundo grupo quase invariavelmente compara e acha o Gemini insuficiente. Eles podem dizer, *“Por que eu usaria o Bard quando o ChatGPT é melhor 90% do tempo?”*. Então, a percepção do usuário casual realmente depende de seu quadro de referência anterior. Aqueles novos em assistentes de IA podem classificar o Gemini como uma novidade útil; aqueles experientes com a concorrência veem como uma decepção que *“ainda é tão ruim”* e precisa melhorar.

- **Usuários Empresariais/Profissionais:** Muitos profissionais deram uma chance ao Bard quando ele foi lançado com integração ao Google Workspace (Duet AI). A percepção entre esse grupo é de ceticismo cauteloso. Por um lado, eles confiam nas promessas empresariais do Google em relação à privacidade de dados e integração (por exemplo, edição de Docs via IA, resumo de reuniões a partir de convites de calendário, etc.). Por outro lado, os primeiros testes frequentemente mostraram o Gemini cometendo erros factuais ou fornecendo saídas genéricas, o que não inspira confiança para uso empresarial. Por exemplo, um profissional pode pedir ao Bard para redigir um relatório para um cliente – se o Bard inserir dados incorretos ou insights fracos, pode ser mais um problema do que uma ajuda. Portanto, usuários profissionais tendem a *pilotar* o Bard em tarefas não críticas, mas ainda confiam no GPT-4 ou Claude para saídas importantes. Há também a percepção de que o Google estava correndo atrás: muitos viram o Bard como “não pronto para o horário nobre” e decidiram esperar. Existe alguma percepção positiva em áreas como **consultas de dados em tempo real** – por exemplo, um analista financeiro no Reddit observou que o Bard poderia buscar informações recentes do mercado graças à pesquisa do Google, o que o ChatGPT não poderia a menos que os plugins estivessem habilitados. Então, em domínios onde dados atuais são fundamentais, alguns profissionais viram uma vantagem. Outra nuance: pessoas no ecossistema do Google (por exemplo, empresas que usam exclusivamente o Google Workspace) têm uma visão ligeiramente mais favorável simplesmente porque o Bard/Gemini é a opção que se encaixa em seu ambiente. Eles estão torcendo para que ele melhore em vez de mudar para um ecossistema totalmente diferente. Em resumo, os usuários empresariais veem o Gemini como *potencialmente muito útil* (dado os dados e a integração de ferramentas do Google), mas até o início de 2025, ele não ganhou confiança total. Eles o percebem como o “novo concorrente que ainda não está lá” – vale a pena monitorar, mas ainda não é uma escolha para tarefas críticas. A reputação do Google compra um pouco de paciência desse público, mas não indefinidamente; se o Gemini não melhorar significativamente, os profissionais podem não adotá-lo amplamente, mantendo-se com outras soluções.

---

## LLMs de Código Aberto (por exemplo, Modelos Baseados em LLaMA)

### Pontos Problemáticos Comuns e Limitações

- **Requisitos de hardware e configuração:** Ao contrário dos chatbots em nuvem, os LLMs de código aberto geralmente exigem que os usuários os executem em hardware local ou em um servidor. Isso apresenta imediatamente um ponto problemático: muitos modelos (por exemplo, um modelo LLaMA de 70 bilhões de parâmetros) precisam de uma GPU poderosa com muita VRAM para rodar suavemente. Como um Redditor colocou sucintamente, *“LLMs locais na maioria do hardware de consumo não terão a precisão necessária para qualquer desenvolvimento complexo.”* Para a pessoa média com apenas uma GPU de 8GB ou 16GB (ou apenas uma CPU), executar um modelo de alta qualidade pode ser lento ou inviável. Os usuários podem recorrer a modelos menores que se encaixam, mas esses frequentemente produzem saídas de qualidade inferior (respostas “mais burras”). A complexidade da configuração é outro problema – instalar pesos de modelo, configurar ambientes como Oobabooga ou LangChain, gerenciar bibliotecas de tokenização, etc., pode ser intimidador para não desenvolvedores. Mesmo usuários tecnicamente habilidosos descrevem como um incômodo acompanhar novas versões de modelos, peculiaridades de drivers de GPU e assim por diante. Um tópico intitulado “Sério, como você realmente usa LLMs locais?” teve pessoas compartilhando que muitos modelos *“ou têm desempenho inferior ou não rodam suavemente no meu hardware”*, e pedindo conselhos práticos.

- **Desempenho inferior aos modelos fechados de última geração:** Os modelos de código aberto fizeram progresso rápido, mas a partir de 2025 muitos usuários notam que ainda ficam atrás dos principais modelos proprietários (GPT-4, Claude) em raciocínio complexo, codificação e precisão factual. Um exemplo vívido: um usuário no r/LocalLLaMA comparou saídas em sua língua nativa e disse *“Todos os outros modelos que tentei falham... Eles nem chegam perto [do GPT-4]. O ChatGPT 4 é absolutamente incrível em escrever”*. Esse sentimento é amplamente ecoado: enquanto modelos abertos menores (como um 13B ou 7B ajustado) podem ser impressionantes para seu tamanho, eles lutam com tarefas que exigem compreensão profunda ou lógica de múltiplas etapas. Mesmo modelos abertos maiores (65B, 70B) que se aproximam do nível GPT-3.5 ainda podem falhar nos tipos de problemas complicados que o GPT-4 lida. Os usuários observam mais alucinações e erros em modelos abertos, especialmente em conhecimento de nicho ou quando os prompts se desviam ligeiramente da distribuição de treinamento. Então, a lacuna em capacidade bruta é um ponto problemático – é preciso temperar as expectativas ao usar modelos locais, o que pode ser frustrante para aqueles acostumados à confiabilidade do ChatGPT.

- **Limite de contexto limitado:** A maioria dos LLMs de código aberto tradicionalmente tem janelas de contexto menores (2048 tokens, talvez 4k tokens) em comparação com o que o ChatGPT ou Claude oferecem. Alguns novos ajustes finos e arquiteturas estão estendendo isso (por exemplo, existem versões de 8K ou 16K tokens do LLaMA-2, e pesquisas como o MPT-7B tinham um contexto de 16K). No entanto, o uso prático de modelos abertos de contexto muito longo ainda está em estágios iniciais. Isso significa que os usuários de modelos locais enfrentam problemas de memória semelhantes – o modelo esquece partes anteriores da conversa ou texto, a menos que implementem esquemas de memória externa (como bancos de dados vetoriais para recuperação). Em discussões no Reddit, os usuários frequentemente mencionam ter que resumir ou truncar manualmente o histórico para permanecer dentro dos limites, o que é trabalhoso. Esta é uma limitação notável, especialmente porque modelos proprietários estão empurrando os comprimentos de contexto ainda mais (como os 100k de Claude).

- **Falta de ajuste fino de instruções em alguns modelos:** Embora muitos modelos abertos sejam ajustados por instrução (Alpaca, LLaMA-2-Chat, etc.), nem todos são tão rigorosamente treinados por RLHF quanto o ChatGPT. Isso pode resultar em modelos locais às vezes sendo menos responsivos a instruções ou prompts do sistema. Por exemplo, um modelo LLaMA bruto apenas continuará o texto e ignorará completamente um formato de prompt de usuário – é preciso usar uma versão ajustada para chat. Mesmo assim, a qualidade dos dados de ajuste importa. Alguns usuários do Reddit notaram que certos modelos de instrução ou *recusavam* excessivamente (porque foram ajustados com segurança pesada, por exemplo, alguns chats do Facebook LLaMA-2 responderiam com recusas de política semelhantes ao ChatGPT) ou *desempenhavam* mal (não seguindo a consulta precisamente). Uma reclamação de usuário no GitHub sobre o CodeLlama-70B-instruct disse que *“é tão censurado que é basicamente inútil”*, mostrando frustração de que um modelo aberto adotou a mesma rigidez sem a alternativa de desligá-lo. Então, dependendo do modelo escolhido, os usuários podem enfrentar um modelo que é muito solto (e dá continuação irrelevante) ou um que é muito estrito/guardado. Obter um comportamento de seguimento de instruções bem equilibrado frequentemente requer tentar múltiplos ajustes finos.

- **Fragmentação e mudança rápida:** O cenário de LLMs de código aberto evolui extremamente rápido, com novos modelos e técnicas (quantização, ajustes finos de LoRA, etc.) surgindo semanalmente. Embora emocionante, isso é um ponto problemático para usuários que não querem ajustar constantemente sua configuração. O que funcionou no mês passado pode estar desatualizado neste mês. Um Redditor humoristicamente comparou isso ao velho oeste, dizendo que a comunidade está *“encontrando maneiras de ‘fingir’ para que pareça semelhante [ao GPT-4]”* mas frequentemente essas são soluções paliativas. Para um usuário casual, é assustador até mesmo escolher entre dezenas de nomes de modelos (Vicuna, Alpaca, Mythomax, Mistral, etc.), cada um com múltiplas versões e forks. Sem uma única plataforma unificada, os usuários dependem de guias da comunidade – que podem ser confusos – para decidir qual modelo atende às suas necessidades. Essa fragmentação em ferramentas e qualidade de modelo é um ponto problemático indireto: eleva a barreira de entrada e o esforço de manutenção.

- **Sem suporte oficial ou garantias:** Quando algo dá errado com um LLM local (por exemplo, o modelo gera conteúdo ofensivo ou trava), não há suporte ao cliente para ligar. Os usuários estão por conta própria ou dependem de ajuda da comunidade. Para entusiastas, isso é bom, mas para uso profissional, essa falta de suporte formal é uma barreira. Alguns usuários do Reddit trabalhando em empresas notaram que, embora adorassem a privacidade de um modelo aberto, se preocupam com quem recorrer se o modelo falhar ou se precisarem de atualizações. Essencialmente, usar código aberto é DIY – tanto uma força quanto uma fraqueza.

### Recursos ou Melhorias Frequentemente Solicitados

- **Melhor eficiência (quantização e otimização):** Um foco importante na comunidade (e, portanto, um pedido comum) é fazer grandes modelos rodarem em hardware menor. Os usuários aguardam ansiosamente técnicas que permitam que um modelo de 70B funcione tão suavemente quanto um modelo de 7B. Já existe quantização de 4 bits ou 8 bits, e os tópicos frequentemente discutem novos métodos como AWQ ou adaptadores semelhantes a RNNs. Um usuário citou pesquisas onde a quantização melhorada poderia manter a qualidade em precisão de bits mais baixa. O desejo é essencialmente: *“Deixe-me rodar um modelo no nível do GPT-4 no meu PC sem lag.”* Cada avanço que se aproxima (como arquiteturas de transformadores mais eficientes ou descarregamento de GPU para CPU) é celebrado. Então, pedidos por melhores ferramentas (como a próxima geração do llama.cpp ou outros aceleradores) são comuns – qualquer coisa para reduzir a barreira de hardware.

- **Modelos maiores e melhores (fechando a lacuna de qualidade):** A comunidade constantemente pressiona por novos modelos de código aberto de última geração. Os usuários estão animados com projetos como o LLaMA 3 (se/quando a Meta lançar um) ou colaborações que poderiam produzir um modelo aberto de 100B+. Muitos expressam otimismo de que *“teremos modelos GPT-4 locais em nossas máquinas até o final deste ano”*. Nessa citação, o usuário aposta no LLaMA 3 mais ajuste fino para entregar desempenho semelhante ao GPT-4. Então, pode-se dizer que um “recurso solicitado” é simplesmente: **mais peso, mais treinamento** – a comunidade quer que empresas de tecnologia ou grupos de pesquisa abram modelos maiores e melhores para que possam rodá-los localmente. Cada vez que um novo modelo (como Mistral 7B ou Falcon 40B) sai, os usuários testam se ele supera o anterior. O pedido final é um modelo aberto que realmente rivaliza com o GPT-4, eliminando a necessidade de IA fechada para aqueles que podem hospedá-lo.

- **Interfaces amigáveis ao usuário e configurações de um clique:** Para ampliar a adoção, muitos usuários pedem maneiras mais fáceis de usar LLMs locais. Isso inclui interfaces GUI onde se pode baixar um modelo e começar a conversar sem trabalho de linha de comando. Existem projetos abordando isso (text-generation-webui do Oobabooga, LM Studio, etc.), mas os novatos ainda lutam. Um tópico recente no Reddit pode perguntar, *“Como configuro um LLM semelhante ao ChatGPT localmente?”*, com usuários pedindo guias passo a passo. Então, um desejo frequente é uma **instalação simplificada** – talvez um aplicativo oficial ou contêiner Docker que agrupe tudo o que é necessário, ou integração em software popular (imagine uma extensão que traga um LLM local para o VSCode ou Chrome facilmente). Essencialmente, reduza a sobrecarga técnica para que pessoas menos experientes em tecnologia também possam desfrutar de LLMs privados.

- **Contexto mais longo e memória para modelos locais:** Desenvolvedores e usuários de código aberto estão experimentando a extensão do contexto (através de ajustes de embedding posicional ou modelos especializados). Muitos usuários pedem que novos modelos venham com janelas de contexto mais longas por padrão – por exemplo, um modelo aberto com 32k de contexto seria muito atraente. Até que isso aconteça, alguns dependem de soluções de “recuperação” externas (LangChain com uma loja vetorial que alimenta informações relevantes no prompt). Usuários no r/LocalLLaMA frequentemente discutem suas configurações para pseudo-contexto longo, mas também expressam desejo de que os próprios modelos lidem com mais. Então, uma melhoria que eles buscam é: *“Nos dê um Claude local – algo com dezenas de milhares de tokens de contexto.”* Isso lhes permitiria fazer análise de livros, conversas longas ou trabalho em grandes bases de código localmente.

- **Ferramentas de ajuste fino melhoradas e personalização de modelos:** Outro pedido é tornar mais fácil ajustar ou personalizar modelos. Embora existam bibliotecas para ajustar modelos em novos dados (Alpaca fez isso com 52K instruções, Low-Rank Adaptation (LoRA) permite ajuste fino com computação limitada, etc.), ainda é um pouco envolvido. Os usuários adorariam ferramentas mais acessíveis para, por exemplo, alimentar todos os seus escritos ou documentos da empresa no modelo e fazê-lo se adaptar. Projetos como LoRA são passos nessa direção, mas uma solução mais automatizada (talvez uma interface de assistente: “faça upload de seus documentos aqui para ajuste fino”) seria bem-vinda. Essencialmente, traga a capacidade que a OpenAI fornece via API (ajuste fino de modelos em dados personalizados) para o reino local de uma maneira amigável ao usuário.

- **Ferramentas de segurança e moderação impulsionadas pela comunidade:** Dado que modelos abertos podem produzir qualquer coisa (incluindo conteúdo não permitido), alguns usuários pediram ou começaram a desenvolver camadas de moderação que os usuários podem alternar ou ajustar. Isso é um pouco nicho, mas a ideia é ter *filtros opcionais* para capturar saídas ofensivas se alguém quiser (por exemplo, se crianças ou estudantes podem interagir com o modelo localmente). Como modelos abertos não se interrompem, ter um plugin ou script para escanear saídas em busca de conteúdo extremo pode ser útil. Alguns na comunidade trabalham em “guardrails éticos” que você pode optar por aderir, o que é interessante porque dá controle ao usuário. Então, recursos em torno de **controlar o comportamento do modelo** – seja para torná-lo mais seguro ou para remover seguranças – são frequentemente discutidos e solicitados, dependendo dos objetivos do usuário.

### Necessidades ou Segmentos de Usuários Desassistidos

- **Usuários não técnicos que valorizam a privacidade:** No momento, os LLMs locais atendem em grande parte a entusiastas de tecnologia. Uma pessoa que não é experiente em computadores, mas se preocupa com a privacidade dos dados (por exemplo, um psicoterapeuta que quer ajuda da IA para analisar notas, mas não pode enviá-las para a nuvem) está desassistida. Eles precisam de uma solução local que seja fácil e segura, mas a complexidade é uma barreira. Até que a IA local se torne tão fácil quanto instalar um aplicativo, esses usuários permanecem à margem – ou comprometendo-se ao usar IA em nuvem e arriscando a privacidade, ou não usando IA de forma alguma. Este segmento – indivíduos preocupados com a privacidade, mas não altamente técnicos – é claramente desassistido pelas ofertas de código aberto atuais.

- **Usuários conscientes do orçamento em regiões com internet ruim:** Outro segmento que se beneficiaria de modelos locais são pessoas que não têm internet confiável ou não podem pagar por chamadas de API. Se alguém pudesse obter um chatbot offline decente em uma máquina de baixo custo, seria valioso (imagine educadores ou estudantes em áreas remotas). Atualmente, a qualidade offline pode não ser ótima, a menos que você tenha um PC de ponta. Existem alguns modelos muito pequenos que rodam em telefones, mas sua capacidade é limitada. Então, usuários que *precisam de IA offline* – devido à conectividade ou custo – são um grupo que o código aberto poderia atender, mas a tecnologia está apenas no limiar de ser útil o suficiente. Eles serão melhor atendidos à medida que os modelos se tornarem mais eficientes.

- **Criadores de conteúdo NSFW ou especializado:** Uma razão pela qual os modelos abertos ganharam popularidade é que eles podem ser sem censura, permitindo casos de uso que as IAs fechadas proíbem (roleplay erótico, exploração de ficção violenta, etc.). Embora este segmento “desassistido” seja controverso, ele é real – muitas comunidades do Reddit (por exemplo, para AI Dungeon ou chatbots de personagens) mudaram para modelos locais após a OpenAI e outros apertarem as regras de conteúdo. Esses usuários agora são atendidos por modelos abertos até certo ponto, mas frequentemente têm que encontrar ou ajustar modelos especificamente para esse propósito (como Mythomax para contar histórias, etc.). Eles ocasionalmente lamentam que muitos modelos abertos ainda tenham resquícios de treinamento de segurança (recusando certas solicitações). Então, eles desejam modelos explicitamente ajustados para criatividade sem censura. Argumentavelmente, eles *estão* sendo atendidos (já que têm soluções), mas não pelos padrões mainstream – eles dependem de forks de comunidade nichados.

- **Comunidades de línguas e culturas:** Modelos de código aberto poderiam ser ajustados para línguas específicas ou conhecimento local, mas a maioria dos proeminentes são centrados no inglês. Usuários de comunidades não falantes de inglês podem estar desassistidos porque nem a OpenAI nem os modelos abertos atendem perfeitamente ao seu idioma/gíria/contexto cultural. Existem esforços (como BLOOM e variantes XLM) para construir modelos abertos multilíngues, e usuários locais pedem ajustes finos em línguas como espanhol, árabe, etc. Se alguém quer um chatbot profundamente fluente em seu dialeto regional ou atualizado sobre notícias locais (em sua língua), os principais modelos podem não entregar. Este é um segmento que modelos abertos *poderiam*